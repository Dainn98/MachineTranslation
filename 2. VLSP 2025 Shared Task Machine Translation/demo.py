import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ================= CONFIG =================
model_path = "output/merged_qwen_3b"
# ============================================

print("â³ Loading model...!")
try:
    tokenizer = AutoTokenizer.from_pretrained(model_path, fix_mistral_regex=True, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map="cuda", dtype=torch.float16, trust_remote_code=True)
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    print("âœ… Model Ä‘Æ°á»£c load thÃ nh cÃ´ng!")
except Exception as e:
    print(f"âŒ Lá»—i load model: {e}")
    exit()

# Translate Function (Pipeline Draft -> Refine)
def translate_pipeline(text, direction):
    if not text: return "", ""
    
    # Determine Translation Mode
    if direction == "English -> Vietnamese":
        task_name = "en2vi"
        src_lang = "English"
        tgt_lang = "Vietnamese"
    else:
        task_name = "vi2en"
        src_lang = "Vietnamese"
        tgt_lang = "English"

    # ================= STEP 1: DRAFTING =================
    # Draft Prompt 
    draft_sys = (
        "You are a professional medical translator. Your task is to translate text deeply and accurately."
        "STRICT RULES:\n"
        "1. DO NOT COPY the input. If the input is short, you STILL MUST translate it.\n"
        "2. DO NOT translate proper names (e.g., Vientiane, New York). Keep them original.\n"
        "3. Convert numbers to the target language format (e.g., 12,5% -> 12.5% in English)."
    )
    
    # Zero-shot
    draft_msgs = [
        {"role": "system", "content": draft_sys},
        {"role": "user", "content": f"Translate {src_lang} to {tgt_lang}: {text}"}
    ]
    
    draft_input = tokenizer.apply_chat_template(draft_msgs, tokenize=False, add_generation_prompt=True)
    
    # Inference Draft
    inputs = tokenizer([draft_input], return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(
            **inputs, 
            max_new_tokens=256, 
            use_cache=True, 
            do_sample=False,
            num_beams=4
        )
    draft_result = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()

    # ================= STEP 2: REFINE =================
    if task_name == "en2vi":
        sys_msg = "You are a senior medical editor. Refine the Vietnamese translation to be more accurate, natural, and use correct medical terminology."
        user_content = f'Original English: "{text}"\nDraft Translation: "{draft_result}"\n\nInstruction: Rewrite it to be perfect.\nRefined Vietnamese:'
        
        # 3-Shot EN->VI
        few_shot = [
            {"role": "user", "content": 'Original English: "The patient was discharged yesterday."\nDraft Translation: "Bá»‡nh nhÃ¢n Ä‘Ã£ Ä‘Æ°á»£c xáº£ ngÃ y hÃ´m qua."\n\nInstruction: Rewrite it.\nRefined Vietnamese:'},
            {"role": "assistant", "content": "Bá»‡nh nhÃ¢n Ä‘Ã£ Ä‘Æ°á»£c xuáº¥t viá»‡n ngÃ y hÃ´m qua."},
            {"role": "user", "content": 'Original English: "No significant past medical history."\nDraft Translation: "KhÃ´ng cÃ³ lá»‹ch sá»­ y táº¿ quÃ¡ khá»© Ä‘Ã¡ng ká»ƒ."\n\nInstruction: Rewrite it.\nRefined Vietnamese:'},
            {"role": "assistant", "content": "Tiá»n sá»­ bá»‡nh lÃ½ khÃ´ng cÃ³ gÃ¬ Ä‘áº·c biá»‡t."},
            {"role": "user", "content": 'Original English: "He complained of severe chest pain."\nDraft Translation: "Anh áº¥y phÃ n nÃ n vá» Ä‘au ngá»±c dá»¯ dá»™i."\n\nInstruction: Rewrite it.\nRefined Vietnamese:'},
            {"role": "assistant", "content": "Bá»‡nh nhÃ¢n than phiá»n Ä‘au ngá»±c dá»¯ dá»™i."}
        ]
        
    else: # vi2en
        sys_msg = "You are a senior medical editor. Refine the English translation to be more accurate, natural, and use correct medical terminology."
        user_content = f'Original Vietnamese: "{text}"\nDraft Translation: "{draft_result}"\n\nInstruction: Rewrite it to be perfect.\nRefined English:'
        
        # 3-Shot VI->EN 
        few_shot = [
            {"role": "user", "content": 'Original Vietnamese: "Bá»‡nh nhÃ¢n nháº­p viá»‡n vÃ¬ khÃ³ thá»Ÿ."\nDraft Translation: "Patient enter hospital because hard breathe."\n\nInstruction: Rewrite it.\nRefined English:'},
            {"role": "assistant", "content": "The patient was admitted to the hospital due to dyspnea."},
            {"role": "user", "content": 'Original Vietnamese: "Bá»‡nh nhÃ¢n Ä‘Ã£ Ä‘Æ°á»£c má»• ruá»™t thá»«a."\nDraft Translation: "Patient was cut appendix."\n\nInstruction: Rewrite it.\nRefined English:'},
            {"role": "assistant", "content": "The patient underwent an appendectomy."},
            {"role": "user", "content": 'Original Vietnamese: "Káº¿t quáº£ xÃ©t nghiá»‡m cho tháº¥y men gan tÄƒng."\nDraft Translation: "Test result show liver enzyme up."\n\nInstruction: Rewrite it.\nRefined English:'},
            {"role": "assistant", "content": "Test results indicated elevated liver enzymes."}
        ]

    refine_msgs = [{"role": "system", "content": sys_msg}] + few_shot + [{"role": "user", "content": user_content}]
    refine_input = tokenizer.apply_chat_template(refine_msgs, tokenize=False, add_generation_prompt=True)

    # Inference Refine
    inputs = tokenizer([refine_input], return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(
            **inputs, 
            max_new_tokens=256, 
            use_cache=True, 
            do_sample=False,
            num_beams=4 
        )
    final_result = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
    
    return draft_result, final_result

# ================= GRADIO UI =================
# Theme
theme = gr.themes.Soft(primary_hue="blue", neutral_hue="slate")

with gr.Blocks(theme=theme, title="Medical Translation Demo") as demo:
    gr.Markdown(
        """
        # ğŸ¥ Medical Translation System (VLSP 2025) by Group 14
        """
    )
    
    with gr.Row():
        # Input
        with gr.Column(scale=1):
            direction = gr.Dropdown(
                choices=["English -> Vietnamese", "Vietnamese -> English"], 
                value="Vietnamese -> English", 
                label="Translation Mode"
            )
            input_text = gr.Textbox(
                lines=5, 
                placeholder="Type medical text here...", 
                label="Input Text"
            )
            
            # Translate Button
            btn_translate = gr.Button("ğŸš€ Dá»ŠCH NGAY (TRANSLATE)", variant="primary")
            
            # Examples
            gr.Examples(
                examples=[
                    ["Bá»‡nh nhÃ¢n nháº­p viá»‡n trong tÃ¬nh tráº¡ng Ä‘au ngá»±c trÃ¡i dá»¯ dá»™i, lan lÃªn vai vÃ  cÃ¡nh tay.", "Vietnamese -> English"],
                    ["Káº¿t quáº£ chá»¥p CT cho tháº¥y cÃ³ khá»‘i u á»Ÿ thÃ¹y phá»•i pháº£i, kÃ­ch thÆ°á»›c 3x4cm.", "Vietnamese -> English"],
                    ["The patient presented with symptoms of acute appendicitis including RLQ pain and fever.", "English -> Vietnamese"],
                    ["Follow-up examination revealed significant improvement in cardiac function.", "English -> Vietnamese"]
                ],
                inputs=[input_text, direction]
            )


        # Output
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ¯ Káº¿t quáº£ dá»‹ch (Translation Result)")
            
            # Main Output
            output_final = gr.Textbox(
                label="âœ¨ Final Output (ÄÃ£ hiá»‡u Ä‘Ã­nh)", 
                interactive=False,
                show_label=True, # Cho nÃºt copy cho tiá»‡n
                lines=5
            )
            
            # Draft Step
            with gr.Accordion("ğŸ” Xem quÃ¡ trÃ¬nh suy luáº­n (Debug / Draft Step)", open=False):
                output_draft = gr.Textbox(
                    label="ğŸ“ Step 1: Draft Translation (Dá»‹ch thÃ´ ban Ä‘áº§u)", 
                    interactive=False,
                    lines=3
                )
                gr.Markdown(
                    "Note: Há»‡ thá»‘ng tá»± Ä‘á»™ng phÃ¡t hiá»‡n lá»—i sai á»Ÿ bÆ°á»›c Draft vÃ  sá»­a láº¡i á»Ÿ bÆ°á»›c Final."
                )

    # Logic
    btn_translate.click(
        fn=translate_pipeline, 
        inputs=[input_text, direction], 
        outputs=[output_draft, output_final]
    )
    
    gr.Markdown("---")
    gr.Markdown("*Developed by ... - Powered by Qwen-3B Finetuned*")

# RUN SERVER
print("ğŸš€ Server Ä‘ang khá»Ÿi Ä‘á»™ng...")
demo.launch(share=True, server_port=7860)
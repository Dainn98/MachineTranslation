from unsloth import FastLanguageModel
import torch
from tqdm import tqdm
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

# ------------------------------------------------------------------------
# C·∫§U H√åNH (S·ª≠a ·ªü ƒë√¢y t√πy t√∫i ti·ªÅn)
# ------------------------------------------------------------------------
max_seq_length = 2048 # D√†i qu√° th√¨ c·∫Øt, 2048 l√† ƒë·ªß cho ƒëo·∫°n vƒÉn y t·∫ø r·ªìi
dtype = None          # ƒê·ªÉ None cho n√≥ t·ª± nh·∫≠n di·ªán (Float16 cho T4, Bfloat16 cho Ampere)
load_in_4bit = True   # B·∫Øt bu·ªôc True ƒë·ªÉ ti·∫øt ki·ªám VRAM

# CH·ªåN MODEL:
model_name = "unsloth/Qwen2.5-3B-Instruct-bnb-4bit"

output_model_name = "output/qwen_mt_3B_v1"

# ------------------------------------------------------------------------
# 1. LOAD MODEL & TOKENIZER
# ------------------------------------------------------------------------
print(f"‚è≥ ƒêang t·∫£i model {model_name}...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

# 2. G·∫ÆN LORA (C√°i n√†y gi√∫p model h·ªçc ƒë∆∞·ª£c m√† kh√¥ng t·ªën nhi·ªÅu VRAM)
model = FastLanguageModel.get_peft_model(
    model,
    r = 64, # S·ªë c√†ng to c√†ng th√¥ng minh nh∆∞ng t·ªën VRAM (16 l√† chu·∫©n b√†i)
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 128,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)

# ------------------------------------------------------------------------
# 3. X·ª¨ L√ù D·ªÆ LI·ªÜU (QUAN TR·ªåNG)
# ------------------------------------------------------------------------
# Load file data 2 chi·ªÅu m√†y v·ª´a t·∫°o
dataset = load_dataset("json", data_files=f"input/final_ultimate_train_utf8.jsonl", split="train")

# H√†m bi·∫øn ƒë·ªïi format messages th√†nh text thu·∫ßn ƒë·ªÉ train
def formatting_prompts_func(examples):
    convos = examples["messages"]
    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]
    return {"text": texts}

# Map d·ªØ li·ªáu
print("üìù ƒêang format d·ªØ li·ªáu...")
dataset = dataset.map(formatting_prompts_func, batched = True)

# ------------------------------------------------------------------------
# 4. C·∫§U H√åNH TRAIN (Hyperparameters)
# ------------------------------------------------------------------------
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # True n·∫øu mu·ªën train nhanh h∆°n cho data l·ªõn, data nh·ªè th√¨ False cho ch·∫Øc
    args = TrainingArguments(
        per_device_train_batch_size = 32,    # TƒÉng l√™n 4 n·∫øu VRAM c√≤n d∆∞
        gradient_accumulation_steps = 2,    # T√≠ch l≈©y gradient
        warmup_steps = 100,
        # max_steps = 1500,
        num_train_epochs = 1,
        learning_rate = 2e-4,
        fp16 = not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 40,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = f"{output_model_name}",
        report_to = "none",
    ),
)

# ------------------------------------------------------------------------
# 5. B·∫§M N√öT TRAIN üöÄ
# ------------------------------------------------------------------------
print("üöÄ B·∫Øt ƒë·∫ßu tu luy·ªán...")
trainer_stats = trainer.train()

# ------------------------------------------------------------------------
# 6. SAVE (QUAN TR·ªåNG NH·∫§T)
# ------------------------------------------------------------------------

print(f"üíæ ƒêang l∆∞u model v√†o th∆∞ m·ª•c: {output_model_name} ...")
model.save_pretrained(output_model_name)
tokenizer.save_pretrained(output_model_name)

# (T√πy ch·ªçn) L∆∞u ƒë·ªãnh d·∫°ng GGUF lu√¥n n·∫øu th√≠ch (b·ªè comment n·∫øu c·∫ßn)
# model.save_pretrained_gguf(output_model_name, tokenizer, quantization_method = "q4_k_m")

print("‚úÖ XONG PHIM! NH·ªö T·∫¢I V·ªÄ NGAY!")
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d60e5e",
   "metadata": {
    "papermill": {
     "duration": 0.008214,
     "end_time": "2025-12-23T08:55:09.046949",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.038735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "557f2c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.061506Z",
     "iopub.status.busy": "2025-12-23T08:55:09.061247Z",
     "iopub.status.idle": "2025-12-23T08:55:09.069131Z",
     "shell.execute_reply": "2025-12-23T08:55:09.068349Z"
    },
    "papermill": {
     "duration": 0.016695,
     "end_time": "2025-12-23T08:55:09.070321",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.053626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "# # ========= General =========\n",
    "import torch\n",
    "SEED: int = 42\n",
    "DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # ========= Paths =========\n",
    "PATH = \"/kaggle/input/iwslt15-englishvietnamese/IWSLT'15 en-vi\"\n",
    "MODEL_NAME: str = \"iwslt_transformer_v1\"\n",
    "\n",
    "# # ========= Dataset =========\n",
    "MAX_SEQ_LEN: int = 100\n",
    "VOCAB_SIZE: int = 30000\n",
    "MIN_FREQ: int = 2\n",
    "\n",
    "# # ========= Model =========\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "D_FF = 2048\n",
    "D_SwiGLU_FF = 1365\n",
    "EPS = 1e-6\n",
    "DROPOUT: float = 0.1\n",
    "\n",
    "# # ========= Training =========\n",
    "BATCH_SIZE: int = 32\n",
    "EPOCHS: int = 30\n",
    "LEANRING_RATE: float = 1e-4\n",
    "PATIENCE: int = 5\n",
    "# label_smoothing: float = 0.0   # giữ để mở rộng sau\n",
    "\n",
    "# # ========= Decoding =========\n",
    "MAX_DECODE_LEN = 80\n",
    "BEAM_SIZE=4\n",
    "LENGTH_PENALTY = 0.6\n",
    "IS_BEAM = True\n",
    "\n",
    "# # beam_size: int = 1             # =1 → greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caef513",
   "metadata": {
    "papermill": {
     "duration": 0.006453,
     "end_time": "2025-12-23T08:55:09.083431",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.076978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c4c1ba1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.098403Z",
     "iopub.status.busy": "2025-12-23T08:55:09.097822Z",
     "iopub.status.idle": "2025-12-23T08:55:09.102157Z",
     "shell.execute_reply": "2025-12-23T08:55:09.101485Z"
    },
    "papermill": {
     "duration": 0.013273,
     "end_time": "2025-12-23T08:55:09.103200",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.089927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helper.py\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from config import SEED\n",
    "\n",
    "def set_seed(seed: int = SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4198b",
   "metadata": {
    "papermill": {
     "duration": 0.006614,
     "end_time": "2025-12-23T08:55:09.116450",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.109836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea95c88",
   "metadata": {
    "papermill": {
     "duration": 0.006478,
     "end_time": "2025-12-23T08:55:09.129475",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.122997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a978d9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.143470Z",
     "iopub.status.busy": "2025-12-23T08:55:09.143269Z",
     "iopub.status.idle": "2025-12-23T08:55:09.147298Z",
     "shell.execute_reply": "2025-12-23T08:55:09.146686Z"
    },
    "papermill": {
     "duration": 0.012478,
     "end_time": "2025-12-23T08:55:09.148332",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.135854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting embedder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile embedder.py\n",
    "import torch.nn as nn\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        print(f'Embedder vocab_size, d_model',vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80a876",
   "metadata": {
    "papermill": {
     "duration": 0.006432,
     "end_time": "2025-12-23T08:55:09.161379",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.154947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Positional_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb7983fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.175972Z",
     "iopub.status.busy": "2025-12-23T08:55:09.175369Z",
     "iopub.status.idle": "2025-12-23T08:55:09.180097Z",
     "shell.execute_reply": "2025-12-23T08:55:09.179328Z"
    },
    "papermill": {
     "duration": 0.013052,
     "end_time": "2025-12-23T08:55:09.181225",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.168173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting positional_encoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile positional_encoder.py\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=200):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i]   = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "\n",
    "        pe = pe.unsqueeze(0) # (max_seq_len, d_model) => (1, max_seq_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "        print(f'PositionalEncoder d_model:', d_model)\n",
    "        print(f'PositionalEncoder max_seq_len:',max_seq_len)\n",
    "\n",
    "    def forward(self, x): #shape input: (batch_size, seq_len, d_model)\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d172b42f",
   "metadata": {
    "papermill": {
     "duration": 0.0074,
     "end_time": "2025-12-23T08:55:09.196717",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.189317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc3b917f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.211212Z",
     "iopub.status.busy": "2025-12-23T08:55:09.210964Z",
     "iopub.status.idle": "2025-12-23T08:55:09.216003Z",
     "shell.execute_reply": "2025-12-23T08:55:09.215200Z"
    },
    "papermill": {
     "duration": 0.013853,
     "end_time": "2025-12-23T08:55:09.217228",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.203375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting attention.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile attention.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from config import EPS, DROPOUT\n",
    "\n",
    "def attention(q, k, v, mask=None, dropout=None):\n",
    "    d_k = q.size(-1)\n",
    "\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "\n",
    "    return torch.matmul(scores, v)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        assert d_model % heads == 0\n",
    "        self.h = heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # linear projection + split into heads\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k).transpose(1,2)\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k).transpose(1,2)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "        # apply attention\n",
    "        scores = attention(q, k, v, mask, self.dropout) #(batch,head,length,d_k)\n",
    "\n",
    "        # concat heads\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model) #(B, L, h, d_k)-> (B, L, D_model) \n",
    "\n",
    "        # output projection\n",
    "        return self.out(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98c887",
   "metadata": {
    "papermill": {
     "duration": 0.006918,
     "end_time": "2025-12-23T08:55:09.231089",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.224171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## feed_forward_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d6690f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.246198Z",
     "iopub.status.busy": "2025-12-23T08:55:09.245622Z",
     "iopub.status.idle": "2025-12-23T08:55:09.250486Z",
     "shell.execute_reply": "2025-12-23T08:55:09.249767Z"
    },
    "papermill": {
     "duration": 0.013557,
     "end_time": "2025-12-23T08:55:09.251507",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.237950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting feed_forward_network.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile feed_forward_network.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from config import DROPOUT,D_FF\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=D_FF, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        return self.linear_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71abfb72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.266792Z",
     "iopub.status.busy": "2025-12-23T08:55:09.266394Z",
     "iopub.status.idle": "2025-12-23T08:55:09.270664Z",
     "shell.execute_reply": "2025-12-23T08:55:09.270106Z"
    },
    "papermill": {
     "duration": 0.012951,
     "end_time": "2025-12-23T08:55:09.271658",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.258707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting swiglu_feed_forward_network.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile swiglu_feed_forward_network.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from config import DROPOUT,D_SwiGLU_FF\n",
    "\n",
    "class SwiGLUFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=D_SwiGLU_FF, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff * 2)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = self.w1(x)\n",
    "        x1, x2 = x_proj.chunk(2, dim=-1)\n",
    "        x = F.silu(x1) * x2   # SwiGLU\n",
    "        x = self.dropout(x)\n",
    "        return self.w2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b35d67",
   "metadata": {
    "papermill": {
     "duration": 0.006639,
     "end_time": "2025-12-23T08:55:09.285144",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.278505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24cfcd06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.299461Z",
     "iopub.status.busy": "2025-12-23T08:55:09.299241Z",
     "iopub.status.idle": "2025-12-23T08:55:09.303756Z",
     "shell.execute_reply": "2025-12-23T08:55:09.303231Z"
    },
    "papermill": {
     "duration": 0.013008,
     "end_time": "2025-12-23T08:55:09.304819",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.291811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting norm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile norm.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from config import EPS\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    # EPS = 1e-6\n",
    "    def __init__(self, d_model, eps=EPS):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias  = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.mean(-1, keepdim=True)\n",
    "        std  = x.std(-1, keepdim=True)\n",
    "        return self.alpha * (x - norm) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c93215",
   "metadata": {
    "papermill": {
     "duration": 0.006981,
     "end_time": "2025-12-23T08:55:09.318839",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.311858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f8521ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.333318Z",
     "iopub.status.busy": "2025-12-23T08:55:09.333097Z",
     "iopub.status.idle": "2025-12-23T08:55:09.338072Z",
     "shell.execute_reply": "2025-12-23T08:55:09.337314Z"
    },
    "papermill": {
     "duration": 0.013613,
     "end_time": "2025-12-23T08:55:09.339204",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.325591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting encoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile encoder.py\n",
    "import torch.nn as nn\n",
    "\n",
    "from embedder import Embedder\n",
    "from positional_encoder import PositionalEncoder\n",
    "from attention import MultiHeadAttention\n",
    "from swiglu_feed_forward_network import SwiGLUFeedForward\n",
    "from norm import Norm\n",
    "from config import DROPOUT\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "\n",
    "        self.attention = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        # self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.ff = SwiGLUFeedForward(d_model, dropout=dropout)\n",
    "        \n",
    "\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attention(x2, x2, x2, mask))\n",
    "\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, heads, dropout) for _ in range(N)\n",
    "        ])\n",
    "\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb684b8c",
   "metadata": {
    "papermill": {
     "duration": 0.006786,
     "end_time": "2025-12-23T08:55:09.352987",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.346201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7954ef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.368294Z",
     "iopub.status.busy": "2025-12-23T08:55:09.367604Z",
     "iopub.status.idle": "2025-12-23T08:55:09.372746Z",
     "shell.execute_reply": "2025-12-23T08:55:09.372113Z"
    },
    "papermill": {
     "duration": 0.014075,
     "end_time": "2025-12-23T08:55:09.373829",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.359754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting decoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile decoder.py\n",
    "\n",
    "import torch.nn as nn\n",
    "from embedder import Embedder\n",
    "from positional_encoder import PositionalEncoder\n",
    "from attention import MultiHeadAttention\n",
    "from swiglu_feed_forward_network import SwiGLUFeedForward\n",
    "from norm import Norm\n",
    "from config import DROPOUT\n",
    "\n",
    "# import norm, attention, feed_forward_network\n",
    "# \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "\n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
    "\n",
    "        # self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.ff = SwiGLUFeedForward(d_model, dropout=dropout)\n",
    "        \n",
    "\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x  = x + self.dropout_1(self.attn_1(x2, x2, x2, tgt_mask))\n",
    "\n",
    "        x2 = self.norm_2(x)\n",
    "        x  = x + self.dropout_2(self.attn_2(x2, enc_out, enc_out, src_mask))\n",
    "\n",
    "        x2 = self.norm_3(x)\n",
    "        x  = x + self.dropout_3(self.ff(x2))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, heads, dropout) for _ in range(N)\n",
    "        ])\n",
    "\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "    def forward(self, tgt, enc_out, src_mask, tgt_mask):\n",
    "        x = self.embed(tgt)\n",
    "        x = self.pe(x)\n",
    "\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, enc_out, src_mask, tgt_mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340c790",
   "metadata": {
    "papermill": {
     "duration": 0.006839,
     "end_time": "2025-12-23T08:55:09.387824",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.380985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de67d367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.403043Z",
     "iopub.status.busy": "2025-12-23T08:55:09.402461Z",
     "iopub.status.idle": "2025-12-23T08:55:09.406817Z",
     "shell.execute_reply": "2025-12-23T08:55:09.406138Z"
    },
    "papermill": {
     "duration": 0.013091,
     "end_time": "2025-12-23T08:55:09.407829",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.394738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile transformer.py\n",
    "\n",
    "import torch.nn as nn\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from config import D_MODEL, NUM_LAYERS, NUM_HEADS, DROPOUT\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab, tgt_vocab, d_model=D_MODEL, N=NUM_LAYERS, heads=NUM_HEADS, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab, d_model, N, heads, dropout)\n",
    "        self.out = nn.Linear(d_model, tgt_vocab)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        e = self.encoder(src, src_mask)\n",
    "        d = self.decoder(tgt, e, src_mask, tgt_mask)\n",
    "        return self.out(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0161e35",
   "metadata": {
    "papermill": {
     "duration": 0.006862,
     "end_time": "2025-12-23T08:55:09.421791",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.414929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76429936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.436334Z",
     "iopub.status.busy": "2025-12-23T08:55:09.436158Z",
     "iopub.status.idle": "2025-12-23T08:55:09.440613Z",
     "shell.execute_reply": "2025-12-23T08:55:09.439958Z"
    },
    "id": "300b3399",
    "papermill": {
     "duration": 0.013032,
     "end_time": "2025-12-23T08:55:09.441663",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.428631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tokenizer.py\n",
    "\n",
    "from collections import Counter\n",
    "from config import VOCAB_SIZE, MIN_FREQ, MAX_SEQ_LEN\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, min_freq=MIN_FREQ, lower=True):\n",
    "        self.lower = lower\n",
    "        self.min_freq = min_freq\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.PAD = \"<pad>\"\n",
    "        self.BOS = \"<bos>\"\n",
    "        self.EOS = \"<eos>\"\n",
    "        self.UNK = \"<unk>\"\n",
    "\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "\n",
    "    def norm(self, text):\n",
    "        return text.lower().strip().split()\n",
    "\n",
    "    def fit(self, texts):\n",
    "        freq = Counter()\n",
    "        for t in texts:\n",
    "            freq.update(self.norm(t))\n",
    "\n",
    "        vocab_words = [w for w, f in freq.items() if f >= self.min_freq]\n",
    "        vocab_words = vocab_words[: self.vocab_size]\n",
    "\n",
    "        vocab = [self.PAD, self.BOS, self.EOS, self.UNK] + vocab_words\n",
    "        self.word2id = {w: i for i, w in enumerate(vocab)}\n",
    "        self.id2word = {i: w for w, i in self.word2id.items()}\n",
    "\n",
    "    def encode(self, text, max_len=MAX_SEQ_LEN):\n",
    "        ids = [self.word2id.get(w, self.word2id[self.UNK]) for w in self.norm(text)]\n",
    "        ids = ids[:max_len]\n",
    "        return [self.word2id[self.BOS]] + ids + [self.word2id[self.EOS]]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        words = []\n",
    "        for i in ids:\n",
    "            w = self.id2word.get(int(i), self.UNK)\n",
    "            if w not in [self.PAD, self.BOS, self.EOS]:\n",
    "                words.append(w)\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def vocab_size_(self):\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def pad_id(self):\n",
    "        return self.word2id[self.PAD]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d9d3e",
   "metadata": {
    "id": "0fd72b40",
    "papermill": {
     "duration": 0.006995,
     "end_time": "2025-12-23T08:55:09.455976",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.448981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### NMT DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00dc0d3",
   "metadata": {
    "papermill": {
     "duration": 0.006984,
     "end_time": "2025-12-23T08:55:09.470300",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.463316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# prep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d92460d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.485118Z",
     "iopub.status.busy": "2025-12-23T08:55:09.484844Z",
     "iopub.status.idle": "2025-12-23T08:55:09.489659Z",
     "shell.execute_reply": "2025-12-23T08:55:09.488960Z"
    },
    "id": "71597d97",
    "papermill": {
     "duration": 0.013619,
     "end_time": "2025-12-23T08:55:09.490733",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.477114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prep_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prep_data.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from config import PATH, MAX_SEQ_LEN\n",
    "\n",
    "def load_iwslt15_text(path=PATH):\n",
    "    train_en = open(os.path.join(path, \"train.en.txt\"), encoding=\"utf8\").read().splitlines()\n",
    "    train_vi = open(os.path.join(path, \"train.vi.txt\"), encoding=\"utf8\").read().splitlines()\n",
    "\n",
    "    dev_en = open(os.path.join(path, \"tst2012.en.txt\"), encoding=\"utf8\").read().splitlines()\n",
    "    dev_vi = open(os.path.join(path, \"tst2012.vi.txt\"), encoding=\"utf8\").read().splitlines()\n",
    "\n",
    "    test_en = open(os.path.join(path, \"tst2013.en.txt\"), encoding=\"utf8\").read().splitlines()\n",
    "    test_vi = open(os.path.join(path, \"tst2013.vi.txt\"), encoding=\"utf8\").read().splitlines()\n",
    "\n",
    "    print(\"Loaded IWSLT15:\")\n",
    "    print(\" - Train:\", len(train_en))\n",
    "    print(\" - Dev  :\", len(dev_en))\n",
    "    print(\" - Test :\", len(test_en))\n",
    "\n",
    "    return (train_en, train_vi), (dev_en, dev_vi), (test_en, test_vi)\n",
    "\n",
    "\n",
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, src_texts, tgt_texts, src_tok, tgt_tok, max_len=MAX_SEQ_LEN):\n",
    "        self.src = src_texts\n",
    "        self.tgt = tgt_texts\n",
    "        self.src_tok = src_tok\n",
    "        self.tgt_tok = tgt_tok\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_ids = self.src_tok.encode(self.src[idx], self.max_len)\n",
    "        tgt_ids = self.tgt_tok.encode(self.tgt[idx], self.max_len)\n",
    "        return torch.LongTensor(src_ids), torch.LongTensor(tgt_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304fe5c",
   "metadata": {
    "id": "006b670c",
    "papermill": {
     "duration": 0.006952,
     "end_time": "2025-12-23T08:55:09.504851",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.497899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### COLLATE + MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2dda1b92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.519640Z",
     "iopub.status.busy": "2025-12-23T08:55:09.519445Z",
     "iopub.status.idle": "2025-12-23T08:55:09.523601Z",
     "shell.execute_reply": "2025-12-23T08:55:09.523059Z"
    },
    "papermill": {
     "duration": 0.012784,
     "end_time": "2025-12-23T08:55:09.524666",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.511882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting collate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile collate.py\n",
    "import torch.nn as nn\n",
    "def collate_batch(batch):\n",
    "    src, tgt = zip(*batch)\n",
    "    src = nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=0)\n",
    "    tgt = nn.utils.rnn.pad_sequence(tgt, batch_first=True, padding_value=0)\n",
    "    return src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3b464d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.539990Z",
     "iopub.status.busy": "2025-12-23T08:55:09.539762Z",
     "iopub.status.idle": "2025-12-23T08:55:09.543892Z",
     "shell.execute_reply": "2025-12-23T08:55:09.543222Z"
    },
    "id": "9d2f6b14",
    "papermill": {
     "duration": 0.01294,
     "end_time": "2025-12-23T08:55:09.544895",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.531955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mask.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mask.py\n",
    "import torch\n",
    "from config import DEVICE\n",
    "def make_src_mask(src):\n",
    "    return (src != 0).unsqueeze(1).unsqueeze(2) # [B,1,1,S]\n",
    "\n",
    "def make_tgt_mask(tgt):\n",
    "    T = tgt.size(1)\n",
    "    pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2) # [B,1,1,T]\n",
    "    seq_mask = torch.tril(torch.ones((T, T), device=DEVICE)).bool()\n",
    "    return pad_mask & seq_mask # broadcast → [B,1,T,T]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdcf595",
   "metadata": {
    "id": "f7bff5ca",
    "papermill": {
     "duration": 0.006955,
     "end_time": "2025-12-23T08:55:09.558957",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.552002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a32631a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.574123Z",
     "iopub.status.busy": "2025-12-23T08:55:09.573706Z",
     "iopub.status.idle": "2025-12-23T08:55:09.577674Z",
     "shell.execute_reply": "2025-12-23T08:55:09.577017Z"
    },
    "id": "9c1e064d",
    "papermill": {
     "duration": 0.012672,
     "end_time": "2025-12-23T08:55:09.578653",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.565981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_one_epoch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_one_epoch.py\n",
    "import torch\n",
    "from config import DEVICE\n",
    "from mask import make_src_mask, make_tgt_mask\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "\n",
    "        tgt_in = tgt[:, :-1]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "\n",
    "        src_mask = make_src_mask(src).to(DEVICE)\n",
    "        tgt_mask = make_tgt_mask(tgt_in).to(DEVICE)\n",
    "\n",
    "        pred = model(src, tgt_in, src_mask, tgt_mask)\n",
    "        pred = pred.reshape(-1, pred.size(-1))\n",
    "        tgt_out = tgt_out.reshape(-1)\n",
    "\n",
    "        loss = criterion(pred, tgt_out)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c1c90",
   "metadata": {
    "id": "8a043d02",
    "papermill": {
     "duration": 0.006957,
     "end_time": "2025-12-23T08:55:09.592690",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.585733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### VALIDATION (BLEU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6691289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:09.607483Z",
     "iopub.status.busy": "2025-12-23T08:55:09.607308Z",
     "iopub.status.idle": "2025-12-23T08:55:14.288893Z",
     "shell.execute_reply": "2025-12-23T08:55:14.288191Z"
    },
    "id": "7c9260c6",
    "outputId": "772bcba8-c75d-4075-88a3-282f6c97ea8d",
    "papermill": {
     "duration": 4.690754,
     "end_time": "2025-12-23T08:55:14.290350",
     "exception": false,
     "start_time": "2025-12-23T08:55:09.599596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in d:\\folder f\\mixtured\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: portalocker in d:\\folder f\\mixtured\\anaconda3\\lib\\site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in d:\\folder f\\mixtured\\anaconda3\\lib\\site-packages (from sacrebleu) (2024.9.11)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in d:\\folder f\\mixtured\\anaconda3\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\folder f\\mixtured\\anaconda3\\lib\\site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\tuan anh\\appdata\\roaming\\python\\python312\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in d:\\folder f\\mixtured\\anaconda3\\lib\\site-packages (from sacrebleu) (5.2.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\tuan anh\\appdata\\roaming\\python\\python312\\site-packages (from portalocker->sacrebleu) (306)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lama-index-core (D:\\Folder F\\Mixtured\\Anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lama-index-core (D:\\Folder F\\Mixtured\\Anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lama-index-core (D:\\Folder F\\Mixtured\\Anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efddc004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:14.307492Z",
     "iopub.status.busy": "2025-12-23T08:55:14.307234Z",
     "iopub.status.idle": "2025-12-23T08:55:18.093153Z",
     "shell.execute_reply": "2025-12-23T08:55:18.092323Z"
    },
    "id": "0a35d4e1",
    "papermill": {
     "duration": 3.796072,
     "end_time": "2025-12-23T08:55:18.094603",
     "exception": false,
     "start_time": "2025-12-23T08:55:14.298531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # %%writefile eval_bleu.py\n",
    "# import sacrebleu\n",
    "# import torch\n",
    "# from config import DEVICE\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate_bleu(model, dataset, src_tok, tgt_tok, max_samples=200, subword=\"sentencepiece\"):\n",
    "#     model.eval()\n",
    "#     hyps = []\n",
    "#     refs = []\n",
    "\n",
    "#     loader = torch.utils.data.DataLoader(\n",
    "#         dataset, batch_size=1, shuffle=False, collate_fn=collate_batch\n",
    "#     )\n",
    "\n",
    "#     for i, (src, tgt) in enumerate(loader):\n",
    "#         if i >= max_samples:\n",
    "#             break\n",
    "\n",
    "#         # ======== SOURCE ========\n",
    "#         src = src.to(DEVICE)\n",
    "#         src_mask = make_src_mask(src)\n",
    "\n",
    "#         # ======== GREEDY DECODE ========\n",
    "#         out_ids = greedy_decode(model, src[0], src_mask[0], tgt_tok)\n",
    "\n",
    "#         # hypothesis decode\n",
    "#         hyp = tgt_tok.decode(out_ids)\n",
    "\n",
    "#         # reference decode\n",
    "#         ref_ids = tgt[0].tolist()\n",
    "#         ref = tgt_tok.decode(ref_ids)\n",
    "\n",
    "#         # ======== DETOKENIZE (SentencePiece, BPE, etc.) ========\n",
    "#         if subword == \"sentencepiece\":\n",
    "#             hyp = hyp.replace(\"▁\", \" \").strip()\n",
    "#             ref = ref.replace(\"▁\", \" \").strip()\n",
    "#         else:\n",
    "#             # nếu bạn dùng tokenizer không phải SP thì để nguyên\n",
    "#             hyp = hyp.strip()\n",
    "#             ref = ref.strip()\n",
    "\n",
    "#         # ======== REMOVE PAD, BOS, EOS NẾU tokenizer còn giữ ========\n",
    "#         # tùy tokenizer của bạn, nhưng nếu SP/BPE thì BOS/EOS là <s> </s>\n",
    "#         for bad in [\"<pad>\", \"<s>\", \"</s>\"]:\n",
    "#             hyp = hyp.replace(bad, \"\").strip()\n",
    "#             ref = ref.replace(bad, \"\").strip()\n",
    "\n",
    "#         hyps.append(hyp)\n",
    "#         refs.append([ref])\n",
    "\n",
    "#     bleu = sacrebleu.corpus_bleu(hyps, refs)\n",
    "#     return bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3b2edda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:18.111038Z",
     "iopub.status.busy": "2025-12-23T08:55:18.110694Z",
     "iopub.status.idle": "2025-12-23T08:55:18.117980Z",
     "shell.execute_reply": "2025-12-23T08:55:18.117403Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.016477,
     "end_time": "2025-12-23T08:55:18.118950",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.102473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate_accuracy_ppl(\n",
    "#     model,\n",
    "#     dataset,\n",
    "#     src_tok,\n",
    "#     tgt_tok,\n",
    "#     batch_size=32,\n",
    "#     max_batches=None\n",
    "# ):\n",
    "#     model.eval()\n",
    "\n",
    "#     pad_id = tgt_tok.pad_id()\n",
    "#     total_correct = 0\n",
    "#     total_tokens = 0\n",
    "#     total_loss = 0.0\n",
    "#     total_batches = 0\n",
    "\n",
    "#     loader = DataLoader(\n",
    "#         dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False,\n",
    "#         collate_fn=collate_batch\n",
    "#     )\n",
    "\n",
    "#     for i, (src, tgt) in enumerate(loader):\n",
    "#         if max_batches is not None and i >= max_batches:\n",
    "#             break\n",
    "\n",
    "#         src = src.to(device)\n",
    "#         tgt = tgt.to(device)\n",
    "\n",
    "#         # ======== SHIFT TARGET ========\n",
    "#         tgt_input = tgt[:, :-1]\n",
    "#         tgt_gold  = tgt[:, 1:]\n",
    "\n",
    "#         src_mask = make_src_mask(src)\n",
    "#         tgt_mask = make_tgt_mask(tgt_input)\n",
    "\n",
    "#         logits = model(src, tgt_input, src_mask, tgt_mask)\n",
    "#         # logits: [B, T, vocab]\n",
    "\n",
    "#         vocab_size = logits.size(-1)\n",
    "#         logits = logits.reshape(-1, vocab_size)\n",
    "#         tgt_gold = tgt_gold.reshape(-1)\n",
    "\n",
    "#         # ======== LOSS (mask PAD) ========\n",
    "#         loss = F.cross_entropy(\n",
    "#             logits,\n",
    "#             tgt_gold,\n",
    "#             ignore_index=pad_id,\n",
    "#             reduction=\"sum\"\n",
    "#         )\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # ======== ACCURACY ========\n",
    "#         preds = logits.argmax(dim=-1)\n",
    "#         mask = tgt_gold != pad_id\n",
    "\n",
    "#         total_correct += (preds[mask] == tgt_gold[mask]).sum().item()\n",
    "#         total_tokens  += mask.sum().item()\n",
    "\n",
    "#         total_batches += 1\n",
    "\n",
    "#     avg_loss = total_loss / total_tokens\n",
    "#     ppl = torch.exp(torch.tensor(avg_loss)).item()\n",
    "#     acc = total_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "\n",
    "#     return {\n",
    "#         \"loss\": avg_loss,\n",
    "#         \"ppl\": ppl,\n",
    "#         \"accuracy\": acc\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d46972",
   "metadata": {
    "id": "c9317837",
    "papermill": {
     "duration": 0.00735,
     "end_time": "2025-12-23T08:55:18.133860",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.126510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### GREEDY DECODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50d403f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:18.149530Z",
     "iopub.status.busy": "2025-12-23T08:55:18.149353Z",
     "iopub.status.idle": "2025-12-23T08:55:18.153788Z",
     "shell.execute_reply": "2025-12-23T08:55:18.153253Z"
    },
    "id": "e10d1466",
    "papermill": {
     "duration": 0.013548,
     "end_time": "2025-12-23T08:55:18.154723",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.141175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing greedy_search_decode.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile greedy_search_decode.py\n",
    "import torch\n",
    "from config import DEVICE, MAX_DECODE_LEN\n",
    "from mask import make_src_mask, make_tgt_mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode(model, src_seq, src_mask, tgt_tok, max_len=MAX_DECODE_LEN):\n",
    "    model.eval()\n",
    "\n",
    "    ys = torch.LongTensor([[tgt_tok.word2id[tgt_tok.BOS]]]).to(DEVICE)\n",
    "    src = src_seq.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        tgt_mask = make_tgt_mask(ys)\n",
    "        out = model(src, ys, make_src_mask(src), tgt_mask)\n",
    "        next_word = out[:, -1, :].argmax(-1).item()\n",
    "\n",
    "        ys = torch.cat([ys, torch.tensor([[next_word]]).to(DEVICE)], dim=1)\n",
    "\n",
    "        if next_word == tgt_tok.word2id[tgt_tok.EOS]:\n",
    "            break\n",
    "\n",
    "    return ys[0].cpu().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b264055",
   "metadata": {
    "id": "db3e9033",
    "papermill": {
     "duration": 0.007399,
     "end_time": "2025-12-23T08:55:18.169601",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.162202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### BEAM SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92fd57b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:18.185607Z",
     "iopub.status.busy": "2025-12-23T08:55:18.185134Z",
     "iopub.status.idle": "2025-12-23T08:55:18.189853Z",
     "shell.execute_reply": "2025-12-23T08:55:18.189290Z"
    },
    "id": "a280d66b",
    "papermill": {
     "duration": 0.01391,
     "end_time": "2025-12-23T08:55:18.190848",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.176938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing beam_search_decode.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile beam_search_decode.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from config import DEVICE, BEAM_SIZE, MAX_DECODE_LEN, LENGTH_PENALTY\n",
    "from mask import make_src_mask, make_tgt_mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_decode(\n",
    "    model,\n",
    "    src_seq,\n",
    "    src_mask,\n",
    "    tgt_tok,\n",
    "    beam_size=BEAM_SIZE,\n",
    "    max_len=MAX_DECODE_LEN,\n",
    "    alpha=LENGTH_PENALTY\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    BOS = tgt_tok.word2id[tgt_tok.BOS]\n",
    "    EOS = tgt_tok.word2id[tgt_tok.EOS]\n",
    "\n",
    "    # src: [1, S]\n",
    "    src = src_seq.unsqueeze(0).to(DEVICE)\n",
    "    src_mask = src_mask.unsqueeze(0).to(DEVICE)  # [1,1,1,S]\n",
    "\n",
    "    # beam = (log_prob, token_ids)\n",
    "    beams = [(0.0, [BOS])]\n",
    "    completed = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "\n",
    "        for log_p, seq in beams:\n",
    "            if seq[-1] == EOS:\n",
    "                completed.append((log_p, seq))\n",
    "                continue\n",
    "\n",
    "            tgt = torch.LongTensor(seq).unsqueeze(0).to(DEVICE)\n",
    "            tgt_mask = make_tgt_mask(tgt)\n",
    "\n",
    "            logits = model(src, tgt, src_mask, tgt_mask)\n",
    "            log_probs = F.log_softmax(logits[:, -1, :], dim=-1).squeeze(0)\n",
    "\n",
    "            topk_log_p, topk_ids = torch.topk(log_probs, beam_size)\n",
    "\n",
    "            for k in range(beam_size):\n",
    "                new_seq = seq + [topk_ids[k].item()]\n",
    "                new_log_p = log_p + topk_log_p[k].item()\n",
    "                new_beams.append((new_log_p, new_seq))\n",
    "\n",
    "        # giữ top beam_size\n",
    "        beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:beam_size]\n",
    "\n",
    "        if len(completed) >= beam_size:\n",
    "            break\n",
    "\n",
    "    candidates = completed if completed else beams\n",
    "\n",
    "    def lp(length):\n",
    "        return ((5 + length) / 6) ** alpha\n",
    "\n",
    "    best = max(\n",
    "        candidates,\n",
    "        key=lambda x: x[0] / lp(len(x[1]))\n",
    "    )\n",
    "\n",
    "    return best[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74389a8",
   "metadata": {
    "id": "64e5f7b8",
    "papermill": {
     "duration": 0.008995,
     "end_time": "2025-12-23T08:55:18.208759",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.199764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "749672b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:18.226121Z",
     "iopub.status.busy": "2025-12-23T08:55:18.225593Z",
     "iopub.status.idle": "2025-12-23T08:55:18.231147Z",
     "shell.execute_reply": "2025-12-23T08:55:18.230517Z"
    },
    "id": "a60eff6c",
    "papermill": {
     "duration": 0.014999,
     "end_time": "2025-12-23T08:55:18.232238",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.217239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sacrebleu\n",
    "import csv\n",
    "\n",
    "from config import DEVICE, BATCH_SIZE\n",
    "from mask import make_src_mask, make_tgt_mask\n",
    "from greedy_search_decode import greedy_decode\n",
    "from beam_search_decode import beam_decode\n",
    "from prep_data import NMTDataset\n",
    "from collate import collate_batch\n",
    "\n",
    "def evaluate_test_metrics(model, test_src, test_tgt,\n",
    "                        src_tok, tgt_tok, max_samples=None,\n",
    "                        bpe_type=\"sentencepiece\",\n",
    "                        save_dir: str = \"./log\",\n",
    "                        log_name: str = \"test_predictions.csv\",\n",
    "                        is_beam = False):\n",
    "    model.eval()\n",
    "    # ====== Prepare log ======\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    log_path = os.path.join(save_dir, log_name)\n",
    "\n",
    "    log_rows = []\n",
    "    log_rows.append([\"input\", \"ground_truth\", \"pred\", \"bleu_score\"])\n",
    "    \n",
    "    # ====== BLEU ======\n",
    "    hyps = []\n",
    "    refs = []\n",
    "\n",
    "    if max_samples is None:\n",
    "        max_samples = len(test_src)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_samples):\n",
    "            # ====== SOURCE ======\n",
    "            src_text = test_src[i]\n",
    "            tgt_text = test_tgt[i]\n",
    "    \n",
    "            # encode EN\n",
    "            src_ids = torch.LongTensor(src_tok.encode(src_text)).unsqueeze(0).to(DEVICE)\n",
    "            src_mask = make_src_mask(src_ids)\n",
    "    \n",
    "            # ====== GREEDY DECODE ======\n",
    "            if is_beam == False:\n",
    "                out_ids = greedy_decode(model, src_ids[0], src_mask[0], tgt_tok)\n",
    "            else:\n",
    "                out_ids = beam_decode(model, src_ids[0], src_mask[0], tgt_tok)\n",
    "            \n",
    "            hyp = tgt_tok.decode(out_ids)\n",
    "    \n",
    "            # ====== DETOKENIZE ======\n",
    "            if bpe_type == \"sentencepiece\":\n",
    "                hyp = hyp.replace(\"▁\", \" \").strip()\n",
    "                ref = tgt_text.replace(\"▁\", \" \").strip()\n",
    "            else:\n",
    "                ref = tgt_text.strip()\n",
    "    \n",
    "            hyps.append(hyp)\n",
    "            refs.append([ref])\n",
    "    \n",
    "            # ====== Sentence BLEU ======\n",
    "            sent_bleu = sacrebleu.sentence_bleu(hyp, [ref]).score\n",
    "    \n",
    "            # ====== Log row ======\n",
    "            log_rows.append([\n",
    "                src_text,\n",
    "                ref,\n",
    "                hyp,\n",
    "                round(sent_bleu, 4)\n",
    "            ])\n",
    "    \n",
    "    # ====== Write CSV ======\n",
    "    with open(log_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(log_rows)\n",
    "        \n",
    "    bleu = sacrebleu.corpus_bleu(hyps, refs).score\n",
    "\n",
    "    # ================= ACC + PPL =================\n",
    "    pad_id = tgt_tok.pad_id()\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    test_ds = NMTDataset(test_src, test_tgt, src_tok, tgt_tok)\n",
    "    loader = torch.utils.data.DataLoader(test_ds, batch_size = BATCH_SIZE, shuffle = False, collate_fn = collate_batch)\n",
    "    \n",
    "    for src, tgt in loader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_in   = tgt[:, :-1]\n",
    "        tgt_gold = tgt[:, 1:]\n",
    "\n",
    "        src_mask = make_src_mask(src)\n",
    "        tgt_mask = make_tgt_mask(tgt_in)\n",
    "\n",
    "        logits = model(src, tgt_in, src_mask, tgt_mask)\n",
    "        # [B, T, V]\n",
    "\n",
    "        vocab_size = logits.size(-1)\n",
    "        logits = logits.reshape(-1, vocab_size)\n",
    "        tgt_gold = tgt_gold.reshape(-1)\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            logits,\n",
    "            tgt_gold,\n",
    "            ignore_index=pad_id,\n",
    "            reduction=\"sum\"\n",
    "        )\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = tgt_gold != pad_id\n",
    "\n",
    "        total_correct += (preds[mask] == tgt_gold[mask]).sum().item()\n",
    "        total_tokens  += mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    ppl = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    acc = total_correct / total_tokens\n",
    "\n",
    "    #Console\n",
    "    print(\n",
    "        f\"TEST BLEU: {bleu:.4f} | \"\n",
    "        f\"TEST PPL: {ppl:.4f} | \"\n",
    "        f\"TEST ACC: {acc:.4f}\"\n",
    "    )\n",
    "    print(f\"Prediction log saved to: {log_path}\")\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu,\n",
    "        \"ppl\": ppl,\n",
    "        \"acc\": acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf405d8b",
   "metadata": {
    "id": "633d4fdb",
    "papermill": {
     "duration": 0.007488,
     "end_time": "2025-12-23T08:55:18.247369",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.239881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ff065c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:18.264504Z",
     "iopub.status.busy": "2025-12-23T08:55:18.263610Z",
     "iopub.status.idle": "2025-12-23T08:55:18.270238Z",
     "shell.execute_reply": "2025-12-23T08:55:18.269437Z"
    },
    "id": "a442aa82",
    "papermill": {
     "duration": 0.016193,
     "end_time": "2025-12-23T08:55:18.271392",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.255199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_full_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_full_pipeline.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from config import DEVICE, MODEL_NAME, EPOCHS, BATCH_SIZE, PATIENCE, D_MODEL, NUM_LAYERS, NUM_HEADS\n",
    "from transformer import Transformer\n",
    "from tokenizer import SimpleTokenizer\n",
    "from prep_data import NMTDataset\n",
    "from collate import collate_batch\n",
    "from train_one_epoch import train_one_epoch\n",
    "from mask import make_src_mask, make_tgt_mask\n",
    "def pretty_params(n):\n",
    "    return f\"{n/1e6:.2f}M\"\n",
    "\n",
    "def train_pipeline(train_src, train_tgt, val_src, val_tgt,\n",
    "                   model_name=MODEL_NAME, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                   patience=PATIENCE):\n",
    "\n",
    "    # === tokenizer ===\n",
    "    src_tok = SimpleTokenizer()\n",
    "    tgt_tok = SimpleTokenizer()\n",
    "    src_tok.fit(train_src)\n",
    "    tgt_tok.fit(train_tgt)\n",
    "\n",
    "    # === datasets ===\n",
    "    train_ds = NMTDataset(train_src, train_tgt, src_tok, tgt_tok)\n",
    "    val_ds   = NMTDataset(val_src,   val_tgt,   src_tok, tgt_tok)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    "    )\n",
    "\n",
    "    # === model ===\n",
    "    model = Transformer(\n",
    "        src_tok.vocab_size_(), tgt_tok.vocab_size_(),\n",
    "        d_model=D_MODEL, N=NUM_LAYERS, heads=NUM_HEADS\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'trainable_params:',pretty_params(trainable_params))\n",
    "    print(f'total_params:',pretty_params(total_params))\n",
    "\n",
    "    print(f'vocab',src_tok.vocab_size_(), tgt_tok.vocab_size_())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=src_tok.word2id[src_tok.PAD])\n",
    "\n",
    "    # === Early Stopping state (dựa trên loss) ===\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    best_path = f\"{model_name}_best.pt\"\n",
    "\n",
    "    # === training loop ===\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        # ========== TRAIN ==========\n",
    "        model.train()\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "\n",
    "        # ========== VALIDATION LOSS ==========\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_loader:\n",
    "                src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "                src_mask = make_src_mask(src)\n",
    "                tgt_input = tgt[:, :-1]     # input\n",
    "                tgt_output = tgt[:, 1:]     # shift for loss\n",
    "                tgt_mask = make_tgt_mask(tgt_input)\n",
    "\n",
    "                logits = model(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "                vocab_size = logits.shape[-1]\n",
    "                loss = criterion(\n",
    "                    logits.reshape(-1, vocab_size),\n",
    "                    tgt_output.reshape(-1)\n",
    "                )\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"\\nEpoch {ep+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ===== EARLY STOPPING BASED ON LOSS =====\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"✔️  Validation loss improved — model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"⚠️  Loss did not improve. Patience = {patience_counter}/{patience}\")\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(\"⛔ Early stopping triggered (no loss improvement).\")\n",
    "                break\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    print(f\"🥇 Best Val Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Model saved at: {best_path}\")\n",
    "\n",
    "    # load best model before returning\n",
    "    model.load_state_dict(torch.load(best_path))\n",
    "\n",
    "    return model, src_tok, tgt_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2f345",
   "metadata": {
    "id": "33f87924",
    "papermill": {
     "duration": 0.008233,
     "end_time": "2025-12-23T08:55:18.288426",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.280193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train IWSLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a02bc7fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:18.305165Z",
     "iopub.status.busy": "2025-12-23T08:55:18.304568Z",
     "iopub.status.idle": "2025-12-23T08:55:18.309375Z",
     "shell.execute_reply": "2025-12-23T08:55:18.308743Z"
    },
    "papermill": {
     "duration": 0.014378,
     "end_time": "2025-12-23T08:55:18.310546",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.296168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import torch\n",
    "\n",
    "from config import *\n",
    "from helper import set_seed\n",
    "from prep_data import load_iwslt15_text\n",
    "from train_full_pipeline import train_pipeline\n",
    "from evaluate import evaluate_test_metrics\n",
    "\n",
    "set_seed()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "print(f'='*80)\n",
    "(train_en, train_vi), (dev_en, dev_vi), (test_en, test_vi) = load_iwslt15_text(PATH)\n",
    "\n",
    "print(f'='*80)\n",
    "model_iwslt, tok_iwslt_en, tok_iwslt_vi = train_pipeline(train_en, train_vi, dev_en, dev_vi, model_name=MODEL_NAME)\n",
    "\n",
    "\n",
    "if IS_BEAM:\n",
    "    print(f'Beam decode')\n",
    "    res = evaluate_test_metrics(model_iwslt, test_en, test_vi, tok_iwslt_en, tok_iwslt_vi,is_beam = True)     \n",
    "else:\n",
    "    print(f'Greedy decode')\n",
    "    res = evaluate_test_metrics(model_iwslt, test_en, test_vi, tok_iwslt_en, tok_iwslt_vi,is_beam = False) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6e468d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:18.328276Z",
     "iopub.status.busy": "2025-12-23T08:55:18.328037Z",
     "iopub.status.idle": "2025-12-23T08:55:18.331180Z",
     "shell.execute_reply": "2025-12-23T08:55:18.330584Z"
    },
    "papermill": {
     "duration": 0.013163,
     "end_time": "2025-12-23T08:55:18.332228",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.319065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b83dc5",
   "metadata": {
    "papermill": {
     "duration": 0.008373,
     "end_time": "2025-12-23T08:55:18.348746",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.340373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39bb140b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:18.368421Z",
     "iopub.status.busy": "2025-12-23T08:55:18.368122Z",
     "iopub.status.idle": "2025-12-23T08:55:18.373574Z",
     "shell.execute_reply": "2025-12-23T08:55:18.372820Z"
    },
    "papermill": {
     "duration": 0.016935,
     "end_time": "2025-12-23T08:55:18.374730",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.357795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demo_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo_train.py\n",
    "import torch\n",
    "\n",
    "from config import *\n",
    "from helper import set_seed\n",
    "from prep_data import load_iwslt15_text\n",
    "from train_full_pipeline import train_pipeline\n",
    "from evaluate import evaluate_test_metrics\n",
    "\n",
    "set_seed()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "print(f'='*80)\n",
    "(train_en, train_vi), (dev_en, dev_vi), (test_en, test_vi) = load_iwslt15_text(PATH)\n",
    "\n",
    "N_TRAIN = 100\n",
    "N_DEV   = 20\n",
    "\n",
    "train_en_small = train_en[:N_TRAIN]\n",
    "train_vi_small = train_vi[:N_TRAIN]\n",
    "\n",
    "dev_en_small = dev_en[:N_DEV]\n",
    "dev_vi_small = dev_vi[:N_DEV]\n",
    "\n",
    "print(f'='*80)\n",
    "# model_iwslt, tok_iwslt_en, tok_iwslt_vi = train_pipeline(train_en, train_vi, dev_en, dev_vi, model_name=MODEL_NAME)\n",
    "model_iwslt, tok_iwslt_en, tok_iwslt_vi = train_pipeline(\n",
    "    train_en_small,\n",
    "    train_vi_small,\n",
    "    dev_en_small,\n",
    "    dev_vi_small,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "print(f'='*80)\n",
    "res = evaluate_test_metrics(model_iwslt, test_en, test_vi, tok_iwslt_en, tok_iwslt_vi,max_samples = 10,is_beam = False) \n",
    "res = evaluate_test_metrics(model_iwslt, test_en, test_vi, tok_iwslt_en, tok_iwslt_vi, max_samples = 2,is_beam = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2279d0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:18.392595Z",
     "iopub.status.busy": "2025-12-23T08:55:18.392356Z",
     "iopub.status.idle": "2025-12-23T08:55:18.397193Z",
     "shell.execute_reply": "2025-12-23T08:55:18.396582Z"
    },
    "papermill": {
     "duration": 0.01503,
     "end_time": "2025-12-23T08:55:18.398247",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.383217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demo_with_cp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo_with_cp.py\n",
    "import torch\n",
    "set_seed()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "(train_en, train_vi), (dev_en, dev_vi), (test_en, test_vi) = load_iwslt15_text()\n",
    "src_tok = SimpleTokenizer()\n",
    "tgt_tok = SimpleTokenizer()\n",
    "src_tok.fit(train_en)\n",
    "tgt_tok.fit(train_vi)\n",
    "\n",
    "model_check = Transformer(\n",
    "        src_tok.vocab_size_(), tgt_tok.vocab_size_(),\n",
    "        d_model=D_MODEL, N=NUM_LAYERS, heads=NUM_HEADS\n",
    "    ).to(device)\n",
    "\n",
    "ckpt_path = \"/kaggle/input/logging-mt/iwslt_transformer_v1_best.pt\"\n",
    "state_dict = torch.load(ckpt_path, map_location=device)\n",
    "model_check.load_state_dict(state_dict)\n",
    "model_check.eval()\n",
    "\n",
    "res = evaluate_test_metrics(model_check, test_en, test_vi, src_tok, tgt_tok,max_samples = 10, is_beam = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e367222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:18.415122Z",
     "iopub.status.busy": "2025-12-23T08:55:18.414850Z",
     "iopub.status.idle": "2025-12-23T08:55:18.419703Z",
     "shell.execute_reply": "2025-12-23T08:55:18.419016Z"
    },
    "papermill": {
     "duration": 0.014518,
     "end_time": "2025-12-23T08:55:18.420723",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.406205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demo_check_bleu.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo_check_bleu.py\n",
    "# !pip install sacrebleu\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "\n",
    "try:\n",
    "    cp_path = \"/kaggle/working/log/test_predictions.csv\"\n",
    "    df = pd.read_csv(cp_path)\n",
    "\n",
    "    hyps = df[\"pred\"].astype(str).tolist()  # list[str]\n",
    "    refs = [[r] for r in df[\"ground_truth\"].astype(str).tolist()]  # list[list[str]]\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, refs)\n",
    "    print(\"Corpus BLEU:\", bleu.score)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File test_predictions.csv không tồn tại\")\n",
    "except KeyError as e:\n",
    "    print(f\"❌ Thiếu cột trong CSV: {e}\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Lỗi khác:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ffafb9",
   "metadata": {
    "papermill": {
     "duration": 0.007758,
     "end_time": "2025-12-23T08:55:18.436425",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.428667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vì sao Whitespace Tokenizer tốt hơn SentencePiece\n",
    "\n",
    "Trong bối cảnh IWSLT’15 EN–VI (≈133k cặp câu, domain TED talks, câu ngắn), việc Whitespace / word-level tokenizer cho kết quả tốt hơn SentencePiece là hiện tượng hợp lý. Nguyên nhân chính đến từ tương tác giữa đặc thù tiếng Việt, kích thước dữ liệu, năng lực mô hình và cách đánh giá bằng BLEU.\n",
    "\n",
    "### 1. Đặc thù tiếng Việt (âm tiết ≈ subword)\n",
    "\n",
    "Ví dụ: “Hà Nội” → [\"Hà\", \"Nội\"].  \n",
    "Về bản chất, mỗi token là âm tiết, gần với subword của một thực thể ngữ nghĩa hoàn chỉnh. Do đó, Whitespace tokenizer vô tình hoạt động giống một subword tokenizer hiệu quả cho tiếng Việt, đặc biệt khi các cụm âm tiết xuất hiện ổn định trong domain TED.\n",
    "\n",
    "### 2. Dữ liệu nhỏ, câu ngắn, domain hẹp\n",
    "\n",
    "- Low-resource: ~133k câu huấn luyện  \n",
    "- Câu ngắn, từ vựng lặp lại nhiều  \n",
    "- Ít OOV thực sự  \n",
    "\n",
    "Với dữ liệu nhỏ, SentencePiece khó học được các quy tắc gộp (merge rules) tối ưu, đặc biệt khi dùng vocab lớn (ví dụ 30k). Kết quả là tokenizer không “nén” được nhiều và đôi khi chỉ chia nhỏ từ một cách không cần thiết.\n",
    "\n",
    "> Lợi thế “không OOV” của SentencePiece không thể hiện rõ trong thiết lập này.\n",
    "\n",
    "### 3. Whitespace tokenizer giữ nguyên đơn vị ngữ nghĩa\n",
    "\n",
    "- Whitespace:  \n",
    "  environmental protection → [\"environmental\", \"protection\"]  \n",
    "- SentencePiece:  \n",
    "  ▁environ ment al ▁protect ion  \n",
    "\n",
    "Với mô hình nhỏ, việc học embedding cho đơn vị ngữ nghĩa hoàn chỉnh dễ hơn so với việc phải tổng hợp nghĩa từ nhiều mảnh subword.\n",
    "\n",
    "### 4. SentencePiece làm chuỗi dài hơn → attention khó hơn\n",
    "\n",
    "- Subword hóa làm độ dài chuỗi tăng (≈1.3–1.8×)  \n",
    "- Self-attention phải xử lý nhiều token hơn → gradient loãng, học khó hơn  \n",
    "- Word-level → chuỗi ngắn, alignment rõ ràng, decode nhanh hơn  \n",
    "\n",
    "Với mô hình nhỏ + ít dữ liệu, chuỗi ngắn thường cho kết quả tốt hơn.\n",
    "\n",
    "### 5. BLEU thiên vị word-level trong trường hợp này\n",
    "\n",
    "BLEU tính điểm dựa trên n-gram sau khi detokenize.  \n",
    "Những lỗi nhỏ ở mức subword dễ làm vỡ n-gram, dẫn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb23d3a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:18.453170Z",
     "iopub.status.busy": "2025-12-23T08:55:18.452635Z",
     "iopub.status.idle": "2025-12-23T08:55:18.459493Z",
     "shell.execute_reply": "2025-12-23T08:55:18.458771Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.016457,
     "end_time": "2025-12-23T08:55:18.460589",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.444132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def print_sample_translations(model, test_src, test_tgt, src_tok, tgt_tok, max_samples=20,bpe_type=\"sentencepiece\"):\n",
    "    print(\"\\n===== MẪU DỊCH THỬ =====\")\n",
    "    model.eval()\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    \n",
    "    if max_samples is None:\n",
    "        max_samples = len(test_src)\n",
    "\n",
    "    for i in range(max_samples):\n",
    "        src_txt = test_src[i]\n",
    "        tgt_txt = test_tgt[i]\n",
    "        \n",
    "        # Encode\n",
    "        src_ids = torch.LongTensor(src_tok.encode(src_txt)).unsqueeze(0).to(device)\n",
    "        src_mask = make_src_mask(src_ids)\n",
    "        \n",
    "        # Decode (Greedy)\n",
    "        out_ids = greedy_decode(model, src_ids[0], src_mask[0], tgt_tok)\n",
    "        pred_res = tgt_tok.decode(out_ids)\n",
    "        \n",
    "         # ====== DETOKENIZE ======\n",
    "        if bpe_type == \"sentencepiece\":\n",
    "            hyp = pred_res.replace(\"▁\", \" \").strip()\n",
    "            ref = tgt_txt.replace(\"▁\", \" \").strip()\n",
    "        else:\n",
    "            ref = tgt_txt.strip()\n",
    "\n",
    "        hyps.append(hyp)\n",
    "        refs.append(ref)\n",
    "        \n",
    "        # Tính BLEU cho câu đơn này (chỉ để tham khảo)\n",
    "        score = sacrebleu.sentence_bleu(hyp, [ref]).score\n",
    "        \n",
    "        print(f\"Input:    {src_txt}\")\n",
    "        print(f\"Target:   {tgt_txt}\")\n",
    "        print(f\"Prediction:  {pred_res}\")    \n",
    "        print(f\"Hypothesis:  {hyp}\")\n",
    "        print(f\"Reference:   {ref}\")\n",
    "        \n",
    "        print(f\"BLEU: {score:.2f}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "    print(f\"TEST BLEU: {bleu:.4f}\")\n",
    "\n",
    "# print_sample_translations(model_iwslt,test_en, test_vi, tok_iwslt_en, tok_iwslt_vi)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c9ee1",
   "metadata": {
    "papermill": {
     "duration": 0.007783,
     "end_time": "2025-12-23T08:55:18.476566",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.468783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6fbd8c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T08:55:18.493279Z",
     "iopub.status.busy": "2025-12-23T08:55:18.492948Z",
     "iopub.status.idle": "2025-12-23T08:55:19.911673Z",
     "shell.execute_reply": "2025-12-23T08:55:19.910610Z"
    },
    "papermill": {
     "duration": 1.429195,
     "end_time": "2025-12-23T08:55:19.913507",
     "exception": false,
     "start_time": "2025-12-23T08:55:18.484312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ File test_predictions.csv không tồn tại\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "try:\n",
    "    cp_path = \"/kaggle/input/logging-mt/test_predictions.csv\"\n",
    "    df = pd.read_csv(cp_path)\n",
    "    \n",
    "    # Plot BLEU score\n",
    "    plt.figure()\n",
    "    plt.hist(df[\"bleu_score\"], bins=20)\n",
    "    plt.xlabel(\"BLEU score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"BLEU score distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "    mean_bleu = df[\"bleu_score\"].mean()\n",
    "    print(\"Average Sentence BLEU:\", mean_bleu)\n",
    "    \n",
    "    df[\"src_len\"] = df[\"input\"].str.split().apply(len)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(df[\"src_len\"], df[\"bleu_score\"])\n",
    "    plt.xlabel(\"Input sentence length\")\n",
    "    plt.ylabel(\"BLEU score\")\n",
    "    plt.title(\"BLEU vs sentence length\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    df[\"tgt_len\"] = df[\"ground_truth\"].str.split().apply(len)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(df[\"tgt_len\"], df[\"bleu_score\"])\n",
    "    plt.xlabel(\"Ground truth sentence length\")\n",
    "    plt.ylabel(\"BLEU score\")\n",
    "    plt.title(\"BLEU vs sentence length\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    df[\"pred_len\"] = df[\"pred\"].str.split().apply(len)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(df[\"pred_len\"], df[\"bleu_score\"])\n",
    "    plt.xlabel(\"Prediction sentence length\")\n",
    "    plt.ylabel(\"BLEU score\")\n",
    "    plt.title(\"BLEU vs sentence length\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(df[\"tgt_len\"], df[\"pred_len\"])\n",
    "    plt.xlabel(\"Ground truth length\")\n",
    "    plt.ylabel(\"Prediction length\")\n",
    "    plt.title(\"Predicted length vs Ground truth length\")\n",
    "    plt.show()\n",
    "    \n",
    "    df.sort_values(\"bleu_score\").head(5)[[\"input\", \"ground_truth\", \"pred\", \"bleu_score\"]]\n",
    "    \n",
    "    df.sort_values(\"bleu_score\", ascending=False).head(5)[[\"input\", \"ground_truth\", \"pred\", \"bleu_score\"]]\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File test_predictions.csv không tồn tại\")\n",
    "except KeyError as e:\n",
    "    print(f\"❌ Thiếu cột trong CSV: {e}\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Lỗi khác:\", e)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2089255,
     "sourceId": 3470456,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8770766,
     "sourceId": 13779451,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.70848,
   "end_time": "2025-12-23T08:55:21.049565",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-23T08:55:05.341085",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3470456,"sourceType":"datasetVersion","datasetId":2089255}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pyvi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:49:07.627389Z","iopub.execute_input":"2025-12-14T03:49:07.627709Z","iopub.status.idle":"2025-12-14T03:49:12.630368Z","shell.execute_reply.started":"2025-12-14T03:49:07.627685Z","shell.execute_reply":"2025-12-14T03:49:12.629603Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"# Đường dẫn dữ liệu\nimport torch\nimport os\n\ndata_path = '/kaggle/input/iwslt15-englishvietnamese/IWSLT\\'15 en-vi/'\ntrain_data_path = '/kaggle/input/iwslt15-englishvietnamese/IWSLT\\'15 en-vi/'\nsaved_model_path = '/kaggle/working/'\nsaved_tokenizer_path = '/kaggle/working/'\ntest_data_path = 'data/test_data/'\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nMAX_SEQ_LEN = 60  # Độ dài tối đa của câu\n\nseed = 42\n\n# Huấn luyện mô hình\nNUM_LAYERS = 6\nD_MODEL = 512\nD_FF = 2048\nEPS = 0.1\nBATCH_SIZE = 164 #---GA params---\nNUM_HEADS = 8\nEPOCHS = 30\nDROPOUT = 0.2 #---GA params---\nCLIP = 1.0\nBATCH_PRINT = 100\n\n#Learning rate\nLEARNING_RATE = 1e-4 #---GA params---\nDECAY_RATE = [1.3, 0.95]\nDECAY_STEP = [3600]\nDECAY_INTERVAL = 390\nWEIGHT_DECAY = 1e-4 #---GA params---\n\nUNKNOWN_TOKEN = '<unk>'\nPAD_TOKEN = '<pad>'\nSTART_TOKEN = '<start>'\nEND_TOKEN = '<end>'\n\n\nPAD_TOKEN_POS = 0\n\n#output\nOUTPUT_DIR = \"output\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nJSON_LOG_PATH = os.path.join(OUTPUT_DIR, \"train.json\")\nCSV_LOG_PATH  = os.path.join(OUTPUT_DIR, \"train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:49:12.632115Z","iopub.execute_input":"2025-12-14T03:49:12.632336Z","iopub.status.idle":"2025-12-14T03:49:16.188859Z","shell.execute_reply.started":"2025-12-14T03:49:12.632310Z","shell.execute_reply":"2025-12-14T03:49:16.188037Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def log_epoch(record, json_path, csv_path):\n    # ---- JSON ----\n    if os.path.exists(json_path):\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            logs = json.load(f)\n    else:\n        logs = []\n\n    logs.append(record)\n\n    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(logs, f, indent=2, ensure_ascii=False)\n\n    # ---- CSV ----\n    write_header = not os.path.exists(csv_path)\n    with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.DictWriter(f, fieldnames=record.keys())\n        if write_header:\n            writer.writeheader()\n        writer.writerow(record)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:49:16.189769Z","iopub.execute_input":"2025-12-14T03:49:16.190102Z","iopub.status.idle":"2025-12-14T03:49:16.195847Z","shell.execute_reply.started":"2025-12-14T03:49:16.190075Z","shell.execute_reply":"2025-12-14T03:49:16.194930Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Loading & Preprocessing Data","metadata":{}},{"cell_type":"code","source":"from pyvi.ViTokenizer import ViTokenizer\nfrom keras.src.legacy.preprocessing.text import Tokenizer\nfrom keras.src.utils import pad_sequences\n\n# Đọc dữ liệu từ tệp\ndef load_data(en_file, vi_file):\n    with open(en_file, 'r', encoding='utf-8') as f:\n        en_data = f.read().strip().split(\"\\n\")\n    with open(vi_file, 'r', encoding='utf-8') as f:\n        vi_data = f.read().strip().split(\"\\n\")\n    return en_data, vi_data\n\ndef get_tokenize(data, add_start_end=False):\n    # Khởi tạo Tokenizer\n    tokenizer = Tokenizer(filters='', oov_token=UNKNOWN_TOKEN)\n    if (add_start_end):\n        tokenizer.fit_on_texts([START_TOKEN, END_TOKEN] + data)\n    else:\n        tokenizer.fit_on_texts(data)\n    return data, tokenizer\n\ndef get_tokenize_seq(en_data, vi_data, en_tokenizer, vi_tokenizer, max_sequence_length):\n    en_data = [f\"{START_TOKEN} {sentence} {END_TOKEN}\" for sentence in en_data]\n    en_sequences = en_tokenizer.texts_to_sequences(en_data)\n\n    vi_data = [ViTokenizer.tokenize(sentence) for sentence in vi_data]\n    vi_sequences = vi_tokenizer.texts_to_sequences(vi_data)\n\n    filtered_en = []\n    filtered_vi = []\n    # Giữ lại những câu có số từ <= max_sequence_length\n    for i in range(len(en_sequences)):\n        if (len(en_sequences[i]) <= max_sequence_length) and (len(vi_sequences[i]) <= max_sequence_length):\n            filtered_en.append(en_sequences[i])\n            filtered_vi.append(vi_sequences[i])\n\n    filtered_en = torch.tensor(pad_sequences(filtered_en, maxlen=max_sequence_length, padding='post'), dtype=torch.long)\n    filtered_vi = torch.tensor(pad_sequences(filtered_vi, maxlen=max_sequence_length, padding='post'), dtype=torch.long)\n\n    return filtered_en, filtered_vi\n\n# Tiền xử lý dữ liệu\ndef preprocess_tokenizer(en_data, vi_data):\n    en_data, en_tokenizer = get_tokenize(en_data, add_start_end=True)\n\n    vi_data = [ViTokenizer.tokenize(sentence) for sentence in vi_data]\n    vi_data, vi_tokenizer = get_tokenize(vi_data)\n\n    return en_tokenizer, vi_tokenizer\n\ndef preprocess_data(train_src_path, train_trg_path, val_src_path, val_trg_path):\n    # Load dữ liệu\n    en_data, vi_data = load_data(train_src_path, train_trg_path)\n    en_data_val, vi_data_val = load_data(val_src_path, val_trg_path)\n\n    en_tokenizer, vi_tokenizer = preprocess_tokenizer(en_data, vi_data)\n\n    en_sequences, vi_sequences = get_tokenize_seq(en_data, vi_data, en_tokenizer, vi_tokenizer,\n                                                  max_sequence_length=MAX_SEQ_LEN)\n    en_val_sequences, vi_val_sequences = get_tokenize_seq(en_data_val, vi_data_val, en_tokenizer, vi_tokenizer,\n                                                          max_sequence_length=MAX_SEQ_LEN)\n\n    all_train_sequences = list(zip(vi_sequences, en_sequences))\n    all_val_sequences = list(zip(vi_val_sequences, en_val_sequences))\n\n    return en_tokenizer, vi_tokenizer, all_train_sequences, all_val_sequences\n\ndef merge_sentences(text, max_seq_length):\n    sentences = [s.strip() for s in text.split(\",\")]  # Tách câu và xóa khoảng trắng dư thừa\n\n    merged = []\n    temp = \"\"\n    word_count = 0\n\n    for sentence in sentences:\n        words = sentence.split()  # Đếm số từ trong câu hiện tại\n        if word_count + len(words) <= max_seq_length:\n            temp = temp + \", \" + sentence if temp else sentence  # Nối câu\n            word_count += len(words)  # Cập nhật số từ\n        else:\n            merged.append(temp)  # Lưu câu hiện tại vào danh sách\n            temp = sentence  # Bắt đầu câu mới\n            word_count = len(words)  # Reset số từ\n\n    if temp:  # Đừng quên thêm câu cuối cùng\n        merged.append(temp)\n\n    return merged","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:49:16.196703Z","iopub.execute_input":"2025-12-14T03:49:16.197020Z","iopub.status.idle":"2025-12-14T03:49:30.358965Z","shell.execute_reply.started":"2025-12-14T03:49:16.196996Z","shell.execute_reply":"2025-12-14T03:49:30.358384Z"}},"outputs":[{"name":"stderr","text":"2025-12-14 03:49:18.136591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765684158.322235      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765684158.395157      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":"# Transformers","metadata":{}},{"cell_type":"code","source":"from torch import nn\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.attention = ScaleDotProductAttention()\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_concat = nn.Linear(d_model, d_model)\n\n    def forward(self, query, key, value, mask=None):\n        # 1. dot product with weight matrices\n        query, key, value = self.w_q(query), self.w_k(key), self.w_v(value)\n\n        # 2. split tensor by number of heads\n        query, key, value = self.split(query), self.split(key), self.split(value)\n\n        # 3. do scale dot product to compute similarity\n        out, attention = self.attention(query, key, value, mask=mask)\n\n        # 4. concat and pass to linear layer\n        out = self.concat(out)\n        out = self.w_concat(out)\n\n        # 5. visualize attention map\n        # TODO : we should implement visualization\n        return out\n\n    def split(self, tensor):\n        batch_size, length, d_model = tensor.size()\n\n        d_tensor = d_model // self.num_heads\n        tensor = tensor.view(batch_size, length, self.num_heads, d_tensor).transpose(1, 2)\n        # it is similar with group convolution (split by number of heads)\n\n        return tensor\n\n    def concat(self, tensor):\n        batch_size, num_heads, length, d_tensor = tensor.size()\n        d_model = d_tensor * self.num_heads\n\n        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n        return tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:49:30.360994Z","iopub.execute_input":"2025-12-14T03:49:30.361800Z","iopub.status.idle":"2025-12-14T03:49:30.368866Z","shell.execute_reply.started":"2025-12-14T03:49:30.361776Z","shell.execute_reply":"2025-12-14T03:49:30.368206Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from torch.optim.lr_scheduler import _LRScheduler\n\nclass CustomLearningRateSchedule(_LRScheduler):\n    def __init__(self, optimizer, initial_lr, decay_rates, decay_steps, lr_decay_interval, last_epoch=-1):\n        \"\"\"\n        initial_lr: Learning rate ban đầu\n        decay_rates: Danh sách hệ số decay (n phần tử)\n        decay_steps: Danh sách step ứng với decay (n-1 phần tử)\n        lr_decay_interval: Khoảng cách giữa các lần decay\n        \"\"\"\n        assert len(decay_rates) - 1 == len(decay_steps), \"Số lượng decay_steps phải ít hơn decay_rates một phần tử\"\n\n        self.initial_lr = initial_lr\n        self.decay_rates = decay_rates\n        self.decay_steps = decay_steps\n        self.lr_decay_interval = lr_decay_interval\n        self.prev_decay_step = 0\n\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        step = self.last_epoch\n        lr = self.initial_lr\n        prev_decay_step = 0\n\n        # Áp dụng các decay ban đầu\n        for i in range(len(self.decay_steps)):\n            decay_factor = self.decay_rates[i]\n            num_intervals = max((min(step, self.decay_steps[i]) - prev_decay_step) // self.lr_decay_interval, 0)\n            lr *= decay_factor ** num_intervals\n            prev_decay_step = self.decay_steps[i]\n\n        # Áp dụng decay cuối cùng mãi mãi\n        decay_factor = self.decay_rates[-1]\n        num_intervals = max((step - prev_decay_step) // self.lr_decay_interval, 0)\n        lr *= decay_factor ** num_intervals\n\n        return [lr for _ in self.base_lrs]  # Trả về danh sách cho từng group của optimizer\n\n    def state_dict(self):\n        return {\n            \"initial_lr\": self.initial_lr,\n            \"decay_rates\": self.decay_rates,\n            \"decay_steps\": self.decay_steps,\n            \"lr_decay_interval\": self.lr_decay_interval,\n            \"prev_decay_step\": self.prev_decay_step\n        }\n\n    def load_state_dict(self, state_dict):\n        self.initial_lr = state_dict[\"initial_lr\"]\n        self.decay_rates = state_dict[\"decay_rates\"]\n        self.decay_steps = state_dict[\"decay_steps\"]\n        self.lr_decay_interval = state_dict[\"lr_decay_interval\"]\n        self.prev_decay_step = state_dict[\"prev_decay_step\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:49:30.369843Z","iopub.execute_input":"2025-12-14T03:49:30.370137Z","iopub.status.idle":"2025-12-14T03:49:30.418404Z","shell.execute_reply.started":"2025-12-14T03:49:30.370110Z","shell.execute_reply":"2025-12-14T03:49:30.417803Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport math\n\nclass ScaleDotProductAttention(nn.Module):\n    def __init__(self):\n        super(ScaleDotProductAttention, self).__init__()\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, query, key, value, mask=None):\n        # input is 4 dimension tensor\n        # [batch_size, num_heads, length, d_tensor]\n        batch_size, num_heads, length, d_tensor = key.size()\n\n        # 1. dot product Query with Key^T to compute similarity\n        key_t = key.transpose(2, 3)\n        score = (query @ key_t) / math.sqrt(d_tensor)\n\n        # 2. apply masking (opt)\n        if mask is not None:\n            score = score.masked_fill(mask == 0, -100000000)\n\n        # 3. pass them softmax to make [0, 1] range\n        score = self.softmax(score)\n\n        # 4. multiply with Value\n        value = score @ value\n\n        return value, score\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len, device):\n        \"\"\"\n           constructor of sinusoid encoding class\n\n           :param d_model: dimension of model\n           :param max_len: max sequence length\n           :param device: hardware device setting\n        \"\"\"\n        super(PositionalEncoding, self).__init__()\n\n        # same size with input matrix (for adding with input matrix)\n        self.encoding = torch.zeros(max_len, d_model, device=device)\n        self.encoding.requires_grad = False # we don't need to compute gradient\n\n        pos = torch.arange(0, max_len, device=device)\n        pos = pos.float().unsqueeze(dim=1)\n\n        _2i = torch.arange(0, d_model, 2, device=device).float()\n\n        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n        # compute positional encoding to consider positional information of words\n\n    def forward(self, x):\n        batch_size, seq_len = x.size()\n        return self.encoding[:seq_len, :]\n\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\nclass TransformerEmbedding(nn.Module):\n    def __init__(self, vocab_size, d_model, max_len, dropout, device):\n        super(TransformerEmbedding, self).__init__()\n        self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_TOKEN_POS)\n        self.pos_emb = PositionalEncoding(d_model, max_len, device)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        tok_emb = self.tok_emb(x)\n        pos_emb = self.pos_emb(x)\n        return self.dropout(tok_emb + pos_emb)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:49:30.419244Z","iopub.execute_input":"2025-12-14T03:49:30.419567Z","iopub.status.idle":"2025-12-14T03:49:30.440168Z","shell.execute_reply.started":"2025-12-14T03:49:30.419543Z","shell.execute_reply":"2025-12-14T03:49:30.439453Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from torch import nn\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, d_ff, num_heads, dropout):\n        super(EncoderLayer, self).__init__()\n        self.attention = MultiHeadAttention(d_model, num_heads)\n        self.norm1 = nn.LayerNorm(d_model, eps=EPS)\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n        self.norm2 = nn.LayerNorm(d_model, eps=EPS)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x, src_mask):\n        # 1. compute self attention\n        _x = x\n        x = self.attention(x, x, x, src_mask)\n\n        # 2. add and norm\n        x = self.dropout1(x)\n        x = self.norm1(_x + x)\n\n        # 3. positionwise feed forward network\n        _x = x\n        x = self.ffn(x)\n\n        # 4. add and norm\n        x = self.dropout2(x)\n        x = self.norm2(_x + x)\n\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self, inp_vocab_size, max_len, d_model, d_ff, num_heads, num_layers, dropout, device):\n        super(Encoder, self).__init__()\n        self.emb = TransformerEmbedding(inp_vocab_size, d_model, max_len, dropout, device=device)\n        self.layers = nn.ModuleList([EncoderLayer(d_model, d_ff, num_heads, dropout) for _ in range(num_layers)])\n\n    def forward(self, src, src_mask):\n        x = self.emb(src)\n        for layer in self.layers:\n            x = layer(x, src_mask)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:49:30.440871Z","iopub.execute_input":"2025-12-14T03:49:30.441211Z","iopub.status.idle":"2025-12-14T03:49:30.460203Z","shell.execute_reply.started":"2025-12-14T03:49:30.441187Z","shell.execute_reply":"2025-12-14T03:49:30.459626Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch import nn\n\nclass Decoder_Layer(nn.Module):\n    def __init__(self, d_model, d_ff, num_heads, dropout):\n        super(Decoder_Layer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.norm1 = nn.LayerNorm(d_model, eps=EPS)\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n        self.norm2 = nn.LayerNorm(d_model, eps=EPS)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.ffn = PositionwiseFeedForward(d_model, d_ff, DROPOUT)\n        self.norm3 = nn.LayerNorm(d_model, eps=EPS)\n        self.dropout3 = nn.Dropout(dropout)\n\n    def forward(self, x, enc_out, trg_mask, src_mask):\n        # 1. compute self attention\n        _x = x\n        x = self.self_attn(x, x, x, mask=trg_mask)\n\n        # 2. add and norm\n        x = self.dropout1(x)\n        x = self.norm1(_x + x)\n\n        if enc_out is not None:\n            # 3. compute encoder - decoder attention\n            _x = x\n            x = self.enc_dec_attn(x, enc_out, enc_out, mask=src_mask)\n\n            # 4. add and norm\n            x = self.dropout2(x)\n            x = self.norm2(_x + x)\n\n        # 5. positionwise feed forward network\n        _x = x\n        x = self.ffn(x)\n\n        # 6. add and norm\n        x = self.dropout3(x)\n        x = self.norm3(_x + x)\n\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, trg_vocab_size, max_len, d_model, d_ff, num_heads, num_layers, dropout, device):\n        super(Decoder, self).__init__()\n        self.embedding = TransformerEmbedding(trg_vocab_size, d_model, max_len, dropout, device)\n        self.layers = nn.ModuleList([Decoder_Layer(d_model, d_ff, num_heads, dropout) for i in range(num_layers)])\n        self.linear = nn.Linear(d_model, trg_vocab_size)\n\n    def forward(self, trg, enc_src, trg_mask, src_mask):\n        trg = self.embedding(trg)\n\n        for layer in self.layers:\n            trg = layer(trg, enc_src, trg_mask, src_mask)\n\n        # pass to LM head\n        output = self.linear(trg)\n\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:49:30.461304Z","iopub.execute_input":"2025-12-14T03:49:30.461604Z","iopub.status.idle":"2025-12-14T03:49:30.478622Z","shell.execute_reply.started":"2025-12-14T03:49:30.461573Z","shell.execute_reply":"2025-12-14T03:49:30.478098Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass Transformer(nn.Module):\n    def __init__(self, src_pad_idx, trg_pad_idx, inp_vocab_size, trg_vocab_size, d_model, num_heads, max_len, d_ff, num_layers, dropout, device):\n        super(Transformer, self).__init__()\n        self.src_pad_idx = src_pad_idx\n        self.trg_pad_idx = trg_pad_idx\n        self.device = device\n\n        self.encoder = Encoder(inp_vocab_size, max_len, d_model, d_ff, num_heads, num_layers, dropout, device)\n        self.decoder = Decoder(trg_vocab_size, max_len, d_model, d_ff, num_heads, num_layers, dropout, device)\n\n    def forward(self, src, trg):\n        src_mask = self.make_src_mask(src)\n        trg_mask = self.make_trg_mask(trg)\n        enc_out = self.encoder(src, src_mask)\n        output = self.decoder(trg, enc_out, trg_mask, src_mask)\n        return output\n\n    def make_src_mask(self, src):\n        src_mask = (src != self.src_pad_idx).unsqueeze(dim=1).unsqueeze(dim=2)\n        return src_mask\n\n    def make_trg_mask(self, trg):\n        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(dim=1).unsqueeze(dim=3)\n        trg_len = trg.shape[1]\n        trg_look_ahead_mask = torch.tril(torch.ones(trg_len, trg_len)).bool().to(self.device)\n        trg_mask = trg_pad_mask & trg_look_ahead_mask\n\n        return trg_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:49:30.479501Z","iopub.execute_input":"2025-12-14T03:49:30.479728Z","iopub.status.idle":"2025-12-14T03:49:30.496062Z","shell.execute_reply.started":"2025-12-14T03:49:30.479713Z","shell.execute_reply":"2025-12-14T03:49:30.495536Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Training & Evaluation","metadata":{}},{"cell_type":"code","source":"import math\nimport time\n\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef initialize_weights(m):\n    if hasattr(m, 'weight') and m.weight.dim() > 1:\n        nn.init.kaiming_uniform(m.weight.data)\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\nen_tokenizer, vi_tokenizer, all_train_sequences, all_val_sequences = preprocess_data(\n                                                                            train_data_path + \"train.en.txt\", train_data_path + \"train.vi.txt\",\n                                                                            data_path + \"tst2013.en.txt\", data_path + \"tst2013.vi.txt\")\n\n# Create training and validation set batches.\ntrain_batches = DataLoader(all_train_sequences, batch_size=BATCH_SIZE, shuffle=True)\nval_batches = DataLoader(all_val_sequences, batch_size=BATCH_SIZE, shuffle=False)\n\n# Kích thước từ vựng\nen_vocab_size = len(en_tokenizer.word_index) + 1\nvi_vocab_size = len(vi_tokenizer.word_index) + 1\n\n# Initializing model\nmodel = Transformer(\n    src_pad_idx=PAD_TOKEN_POS,\n    trg_pad_idx=PAD_TOKEN_POS,\n    d_model=D_MODEL,\n    inp_vocab_size=vi_vocab_size,\n    trg_vocab_size=en_vocab_size,\n    max_len=MAX_SEQ_LEN,\n    d_ff=D_FF,\n    num_heads=NUM_HEADS,\n    num_layers=NUM_LAYERS,\n    dropout=DROPOUT,\n    device=DEVICE\n).to(DEVICE)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')\n\n# Tạo optimizer\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\nscheduler = CustomLearningRateSchedule(\n    optimizer=optimizer,\n    initial_lr=LEARNING_RATE,\n    decay_rates=DECAY_RATE,\n    decay_steps=DECAY_STEP,\n    lr_decay_interval=DECAY_INTERVAL\n)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_POS)\n\ndef train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    for (i, (src, trg)) in enumerate(iterator):\n        src = src.to(model.device)  # Đưa src về cùng thiết bị với model\n        trg = trg.to(model.device)  # Đưa trg về cùng thiết bị với model\n        optimizer.zero_grad()\n        output = model(src, trg[:, :-1])\n        output_reshape = output.contiguous().view(-1, output.shape[-1])\n        trg = trg[:, 1:].contiguous().view(-1)\n\n        loss = criterion(output_reshape, trg)\n        loss.backward()\n        # Tính norm của gradient trước khi clip\n        grad_norm_before = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n        # Clip gradient để tránh exploding gradient\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        # Tính norm của gradient sau khi clip\n        grad_norm_after = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n        optimizer.step()\n        scheduler.step()\n\n        # Tính số lượng token đúng\n        pred = output.argmax(dim=-1).view(-1)  # Lấy token có xác suất cao nhất\n        mask = (trg != PAD_TOKEN_POS)  # Bỏ qua token padding\n        correct = (pred == trg) & mask  # Đúng và không phải padding\n        total_correct += correct.sum().item()\n        total_tokens += mask.sum().item()\n        \n        epoch_loss += loss.item()\n        if (i + 1) % BATCH_PRINT == 0:\n            lr = optimizer.param_groups[0]['lr']\n            print(f'Batch: {i+1}/{len(iterator)}, Loss: {loss.item():.4f}, Accuracy: {total_correct / total_tokens:.4f}, LR: {lr:.6f}, '\n                  f'Grad Norm Before Clip: {grad_norm_before:.6f}, Grad Norm After Clip: {grad_norm_after:.6f}')\n            \n    return epoch_loss / len(iterator), total_correct / total_tokens\n\ndef evaluate(model, iterator, criterion):\n    model.eval()\n    epoch_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    with torch.no_grad():\n        for (i, (src, trg)) in enumerate(iterator):\n            src = src.to(model.device)  # Đưa src về cùng thiết bị với model\n            trg = trg.to(model.device)  # Đưa trg về cùng thiết bị với model\n            output = model(src, trg[:, :-1])\n            output_reshape = output.contiguous().view(-1, output.shape[-1])\n            trg = trg[:, 1:].contiguous().view(-1)\n\n            # Tính số lượng token đúng\n            pred = output.argmax(dim=-1).view(-1)  # Lấy token có xác suất cao nhất\n            mask = (trg != PAD_TOKEN_POS)  # Bỏ qua token padding\n            correct = (pred == trg) & mask  # Đúng và không phải padding\n            total_correct += correct.sum().item()\n            total_tokens += mask.sum().item()\n\n            loss = criterion(output_reshape, trg)\n            epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator), total_correct / total_tokens\n\ndef run(total_epoch, best_loss):\n    train_losses, test_losses = [], []\n    for step in range(total_epoch):\n        print(f'Epoch: {step + 1}')\n        start_time = time.time()\n        train_loss, train_accuracy = train(model, train_batches, optimizer, criterion, CLIP)\n        val_loss, val_accuracy = evaluate(model, val_batches, criterion)\n        end_time = time.time()\n\n        train_losses.append(train_loss)\n        test_losses.append(val_loss)\n\n        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n        if val_loss < best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), f'{saved_model_path}/model-{val_loss:.3f}-{val_accuracy:.3f}.pt')\n\n        # Log record\n        log_record = {\n            \"epoch\": step + 1,\n            \"train_loss\": round(train_loss, 6),\n            \"train_accuracy\": round(train_accuracy, 6),\n            \"train_ppl\": round(math.exp(train_loss), 6),\n            \"val_loss\": round(val_loss, 6),\n            \"val_accuracy\": round(val_accuracy, 6),\n            \"val_ppl\": round(math.exp(val_loss), 6),\n            \"epoch_time_sec\": round(end_time - start_time, 2)\n        }\n\n        log_epoch(log_record, JSON_LOG_PATH, CSV_LOG_PATH)\n\n        #Console\n        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train Accuracy: {train_accuracy:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n        print(f'\\tVal Loss: {val_loss:.3f} | Val Accuracy: {val_accuracy:.3f} |  Val PPL: {math.exp(val_loss):7.3f}')\n\n# run(total_epoch=EPOCHS, best_loss=float('inf'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:49:30.496755Z","iopub.execute_input":"2025-12-14T03:49:30.496990Z","iopub.status.idle":"2025-12-14T03:51:03.088248Z","shell.execute_reply.started":"2025-12-14T03:49:30.496970Z","shell.execute_reply":"2025-12-14T03:51:03.087456Z"}},"outputs":[{"name":"stdout","text":"The model has 111,941,877 trainable parameters\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from deap import base, creator, tools, algorithms\nimport random\n\ndef train_model(learning_rate, dropout, weight_decay, batch_size, quick_epoch=True):\n    \"\"\"\n    Optimize performance with GA\n    \"\"\"\n    # Tao dataloader voi batch_size\n    train_loader = DataLoader(all_train_sequences, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(all_val_sequences, batch_size=batch_size,shuffle=False)\n\n    #Khoi tao model Transformer voi dropout\n    model = Transformer(\n        src_pad_idx=PAD_TOKEN_POS,\n        trg_pad_idx=PAD_TOKEN_POS,\n        inp_vocab_size=vi_vocab_size,\n        trg_vocab_size=en_vocab_size,\n        d_model=D_MODEL,\n        num_heads=NUM_HEADS,\n        max_len=MAX_SEQ_LEN,\n        d_ff=D_FF,\n        num_layers=NUM_LAYERS,\n        dropout=dropout,\n        device=DEVICE\n    ).to(DEVICE)\n\n    #khoi tao optim, scheduler voi learning_rate, weight_decay\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    scheduler = CustomLearningRateSchedule(\n        optimizer=optimizer,\n        initial_lr=learning_rate,\n        decay_rates=DECAY_RATE,\n        decay_steps=DECAY_STEP,\n        lr_decay_interval=DECAY_INTERVAL\n    )\n\n    criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_POS)\n\n    epochs = 10\n    for epoch in range(epochs):\n        train(model, train_loader, optimizer, criterion, CLIP)\n    # Đánh giá accuracy trên tập validation\n    _, val_acc = evaluate(model, val_loader, criterion)\n    return val_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:51:41.949297Z","iopub.execute_input":"2025-12-14T03:51:41.950076Z","iopub.status.idle":"2025-12-14T03:51:41.958924Z","shell.execute_reply.started":"2025-12-14T03:51:41.950021Z","shell.execute_reply":"2025-12-14T03:51:41.958190Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\n# -------------------------------------------------\n# 1. Define the fitness of an individual\n# -------------------------------------------------\n# Fitness: maximize validation accuracy\n\nrandom.seed(seed)\ntorch.manual_seed(seed)\n\ncreator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\ncreator.create(\"Individual\", list, fitness=creator.FitnessMax)\n\n\n# -------------------------------------------------\n# 2. Hyperparameter space\n# -------------------------------------------------\ndef create_individual():\n    return creator.Individual([\n        random.uniform(1e-5, 5e-4),   # learning_rate\n        random.uniform(0.05, 0.3),    # dropout\n        random.choice([1e-5,2e-5,5e-5,1e-4,2e-4,5e-4]),     # weight_decay\n        random.choice([128, 144, 160, 176, 164,192, 208, 224, 256])   # batch_size\n    ])\n\ndef cx_individual(ind1, ind2):\n    # lr (float)\n    ind1[0], ind2[0] = tools.cxBlend(ind1[0:1], ind2[0:1], alpha=0.5)\n    \n    # dropout (float)\n    ind1[1], ind2[1] = tools.cxBlend(ind1[1:2], ind2[1:2], alpha=0.5)\n\n    # weight_decay (discrete)\n    if random.random() < 0.5:\n        ind1[2], ind2[2] = ind2[2], ind1[2]\n\n    # batch_size (int)\n    if random.random() < 0.5:\n        ind1[3], ind2[3] = ind2[3], ind1[3]\n\n    return ind1, ind2\n\ndef mut_individual(ind):\n    # learning_rate\n    if random.random() < 0.3:\n        ind[0] *= random.uniform(0.5, 1.5)\n        ind[0] = max(1e-5, min(ind[0], 5e-4))\n\n    # dropout\n    if random.random() < 0.3:\n        ind[1] += random.uniform(-0.05, 0.05)\n        ind[1] = max(0.05, min(ind[1], 0.3))\n\n    # weight_decay\n    if random.random() < 0.2:\n        ind[2] = random.choice([1e-5,2e-5,5e-5,1e-4,2e-4,5e-4])\n\n    # batch_size\n    if random.random() < 0.2:\n        ind[3] = random.choice([128, 144, 160, 176, 192, 208, 224, 256])\n\n    return (ind,)\n\n    \n# -------------------------------------------------\n# 3. Evaluation function (train few epochs)\n# -------------------------------------------------\ndef evaluate(individual):\n    lr, dropout, wd, bs = individual\n\n    lr = float(lr)\n    dropout = float(dropout)\n    wd = float(wd)\n    bs = int(bs)\n\n    # Call your train() for 1–3 quick epochs\n    val_acc = train_model(\n        learning_rate=lr,\n        dropout=dropout,\n        weight_decay=wd,\n        batch_size=bs,\n        quick_epoch=True\n    )\n\n    return (val_acc,)\n\n\n# -------------------------------------------------\n# 4. GA setup\n# -------------------------------------------------\ntoolbox = base.Toolbox()\ntoolbox.register(\"individual\", create_individual)\ntoolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\ntoolbox.register(\"evaluate\", evaluate)\ntoolbox.register(\"mate\", cx_individual)\ntoolbox.register(\"mutate\",mut_individual)\ntoolbox.register(\"select\", tools.selTournament, tournsize=3)\n\n# -------------------------------------------------\n# 5. Run GA\n# -------------------------------------------------\ndef run_ga():\n    pop = toolbox.population(n=20)   # 20 candidates\n    ngen = 10                        # 10 generations\n\n    algorithms.eaSimple(\n        pop,\n        toolbox,\n        cxpb=0.5,      # crossover probability\n        mutpb=0.3,     # mutation probability\n        ngen=ngen,\n        verbose=True\n    )\n\n    best = tools.selBest(pop, k=1)[0]\n    print(\"Best hyperparameters:\", best)\n    return best\n\nbest_params = run_ga()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:52:03.658552Z","iopub.execute_input":"2025-12-14T03:52:03.659308Z","iopub.status.idle":"2025-12-14T04:04:19.361077Z","shell.execute_reply.started":"2025-12-14T03:52:03.659270Z","shell.execute_reply":"2025-12-14T04:04:19.360021Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Batch: 100/795, Loss: 5.3139, Accuracy: 0.1699, LR: 0.000339, Grad Norm Before Clip: 0.865263, Grad Norm After Clip: 0.865263\nBatch: 200/795, Loss: 4.7659, Accuracy: 0.2169, LR: 0.000339, Grad Norm Before Clip: 0.991328, Grad Norm After Clip: 0.991328\nBatch: 300/795, Loss: 4.4438, Accuracy: 0.2527, LR: 0.000339, Grad Norm Before Clip: 1.213187, Grad Norm After Clip: 0.999999\nBatch: 400/795, Loss: 4.2633, Accuracy: 0.2788, LR: 0.000339, Grad Norm Before Clip: 0.868490, Grad Norm After Clip: 0.868490\nBatch: 500/795, Loss: 3.9699, Accuracy: 0.2991, LR: 0.000339, Grad Norm Before Clip: 1.059566, Grad Norm After Clip: 0.999999\nBatch: 600/795, Loss: 3.8776, Accuracy: 0.3156, LR: 0.000339, Grad Norm Before Clip: 0.965310, Grad Norm After Clip: 0.965310\nBatch: 700/795, Loss: 3.5537, Accuracy: 0.3298, LR: 0.000339, Grad Norm Before Clip: 0.921641, Grad Norm After Clip: 0.921641\nBatch: 100/795, Loss: 3.3295, Accuracy: 0.4511, LR: 0.000339, Grad Norm Before Clip: 0.908435, Grad Norm After Clip: 0.908435\nBatch: 200/795, Loss: 3.1474, Accuracy: 0.4560, LR: 0.000339, Grad Norm Before Clip: 1.008217, Grad Norm After Clip: 0.999999\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1283548127.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_ga\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_47/1283548127.py\u001b[0m in \u001b[0;36mrun_ga\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mngen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m                        \u001b[0;31m# 10 generations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     algorithms.eaSimple(\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtoolbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deap/algorithms.py\u001b[0m in \u001b[0;36meaSimple\u001b[0;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0minvalid_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1283548127.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(individual)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Call your train() for 1–3 quick epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     val_acc = train_model(\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/702712290.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(learning_rate, dropout, weight_decay, batch_size, quick_epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Đánh giá accuracy trên tập validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3050749298.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Tính norm của gradient trước khi clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mgrad_norm_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Clip gradient để tránh exploding gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3050749298.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Tính norm của gradient trước khi clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mgrad_norm_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Clip gradient để tránh exploding gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[1;32m    870\u001b[0m                 \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             )\n\u001b[0;32m--> 872\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1803\u001b[0m             ):\n\u001b[1;32m   1804\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m                     return torch.linalg.vector_norm(\n\u001b[0m\u001b[1;32m   1806\u001b[0m                         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m                     )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"# # Huấn luyện mô hình cuối cùng với các siêu tham số tốt nhất\n# BATCH_SIZE = best_bs\n# LEARNING_RATE = best_lr\n# WEIGHT_DECAY = best_wd\n# DROPOUT = best_dropout\n\n# # Tạo lại DataLoader với batch size tốt nhất\n# train_batches = DataLoader(all_train_sequences, batch_size=BATCH_SIZE, shuffle=True)\n# val_batches = DataLoader(all_val_sequences, batch_size=BATCH_SIZE, shuffle=False)\n\n# # Khởi tạo lại mô hình với dropout tốt nhất\n# model = Transformer(\n#     src_pad_idx=PAD_TOKEN_POS,\n#     trg_pad_idx=PAD_TOKEN_POS,\n#     inp_vocab_size=vi_vocab_size,\n#     trg_vocab_size=en_vocab_size,\n#     d_model=D_MODEL,\n#     num_heads=NUM_HEADS,\n#     max_len=MAX_SEQ_LEN,\n#     d_ff=D_FF,\n#     num_layers=NUM_LAYERS,\n#     dropout=DROPOUT,\n#     device=DEVICE\n# ).to(DEVICE)\n# optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n# scheduler = CustomLearningRateSchedule(\n#     optimizer=optimizer,\n#     initial_lr=LEARNING_RATE,\n#     decay_rates=DECAY_RATE,\n#     decay_steps=DECAY_STEP,\n#     lr_decay_interval=DECAY_INTERVAL\n# )\n# criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_POS)\n\n# # Chạy huấn luyện đầy đủ với EPOCHS\n# run(total_epoch=EPOCHS, best_loss=float('inf'))\n\n# # Tính và báo cáo độ chính xác trên tập validation cuối cùng\n# _, final_val_acc = evaluate(model, val_batches, criterion)\n# print(f\"Final validation accuracy with best hyperparameters: {final_val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T03:51:03.184432Z","iopub.status.idle":"2025-12-14T03:51:03.184641Z","shell.execute_reply.started":"2025-12-14T03:51:03.184533Z","shell.execute_reply":"2025-12-14T03:51:03.184542Z"}},"outputs":[],"execution_count":null}]}
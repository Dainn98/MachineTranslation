{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implementation","metadata":{}},{"cell_type":"code","source":"!pip install -q pyvi\n!pip install -q sacrebleu\n!git clone https://github.com/Dainn98/MachineTranslation.git\n%cd MachineTranslation/data/\"IWSLT'15 en-vi\"\n!ls","metadata":{"execution":{"iopub.status.busy":"2025-12-23T10:12:46.887352Z","iopub.execute_input":"2025-12-23T10:12:46.887861Z","iopub.status.idle":"2025-12-23T10:12:54.886394Z","shell.execute_reply.started":"2025-12-23T10:12:46.887835Z","shell.execute_reply":"2025-12-23T10:12:54.885700Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'MachineTranslation'...\nremote: Enumerating objects: 68, done.\u001b[K\nremote: Counting objects: 100% (68/68), done.\u001b[K\nremote: Compressing objects: 100% (59/59), done.\u001b[K\nremote: Total 68 (delta 9), reused 62 (delta 6), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (68/68), 10.08 MiB | 18.32 MiB/s, done.\nResolving deltas: 100% (9/9), done.\n/kaggle/working/MachineTranslation/data/IWSLT'15 en-vi/MachineTranslation/data/IWSLT'15 en-vi\ndict.en-vi.txt\t\t   train.vi.txt    tst2013.en.txt  vocab.vi.txt\nluong-manning-iwslt15.pdf  tst2012.en.txt  tst2013.vi.txt\ntrain.en.txt\t\t   tst2012.vi.txt  vocab.en.txt\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"# %%writefile config.py\nimport torch\nimport os\nfrom datetime import datetime\n\ndata_path = '/kaggle/input/iwslt15-englishvietnamese/IWSLT\\'15 en-vi/'\ntrain_data_path = '/kaggle/input/iwslt15-englishvietnamese/IWSLT\\'15 en-vi/'\nsaved_model_path = '/kaggle/working/'\nsaved_tokenizer_path = '/kaggle/working/'\ntest_data_path = 'data/test_data/'\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nvocab_src_path = 'vocab_en.json'\nvocab_tgt_path = 'vocab_vi.json'\nw2v_src_path = 'w2v_en.model'\nw2v_tgt_path = 'w2v_vi.model'\n\nMAX_SEQ_LEN = 60  # Độ dài tối đa của câu\n\nBEAM_SIZE = 4 # We used beam search with a beam size of 4 and length penalty α = 0.6 / Attention is All you need\nLENGTH_PENALTY = 0.6\n\n#Loss params\nlabel_smoothing=0.1\n#optimizer params\nBETAS = (.9,.98)\nEPSILON = 1e-9\nWARMUP_RATIO=.1\n\n\n# Huấn luyện mô hình\nEPOCHS = 30\nFREEZE_EPOCHS = 2\nSKIPGRAM_EPOCHS = 6\nSKIPGRAM_DIM = 300\nNUM_LAYERS = 6\nD_MODEL = 512\nD_FF = int(4/3 * D_MODEL)\nEPS = 0.1\nBATCH_SIZE = 164\nNUM_HEADS = 8\nDROPOUT = 0.2\nCLIP = 1.0\nBATCH_PRINT = 100\nDEBUG = True # demo traing\n#DEBUG = False \n\n\n#Learning rate\nLEARNING_RATE = 3e-5\nDECAY_RATE = [1.3, 0.95]\nDECAY_STEP = [3600]\nDECAY_INTERVAL = 390\nWEIGHT_DECAY = 1e-4\n\nUNKNOWN_TOKEN = '<unk>'\nPAD_TOKEN = '<pad>'\nSTART_TOKEN = '<start>'\nEND_TOKEN = '<end>'\n\n\nPAD_TOKEN_POS = 0\n\n#output\nOUTPUT_DIR = \"output\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nRUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\nJSON_LOG_PATH = os.path.join(OUTPUT_DIR, f\"training_{RUN_ID}.json\")\nCSV_LOG_PATH  = os.path.join(OUTPUT_DIR, f\"training_{RUN_ID}.csv\")\n\n\nCSV_FIELDS = [\n    \"epoch\",\n    \"train_loss\",\n    \"val_loss\",\n    \"train_accuracy\",\n    \"val_accuracy\",\n    # \"val_bleu\",\n    \"train_ppl\",\n    \"val_ppl\",\n    \"epoch_time_sec\"\n]","metadata":{"execution":{"iopub.status.busy":"2025-12-23T10:13:20.907887Z","iopub.execute_input":"2025-12-23T10:13:20.908760Z","iopub.status.idle":"2025-12-23T10:13:20.918195Z","shell.execute_reply.started":"2025-12-23T10:13:20.908725Z","shell.execute_reply":"2025-12-23T10:13:20.917494Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"!wc -l train.en.txt\n!wc -c train.en.txt\n\n!wc -l train.vi.txt\n!wc -c train.vi.txt","metadata":{"execution":{"iopub.status.busy":"2025-12-23T10:13:23.629671Z","iopub.execute_input":"2025-12-23T10:13:23.630465Z","iopub.status.idle":"2025-12-23T10:13:24.132661Z","shell.execute_reply.started":"2025-12-23T10:13:23.630425Z","shell.execute_reply":"2025-12-23T10:13:24.131755Z"},"trusted":true},"outputs":[{"name":"stdout","text":"133317 train.en.txt\n13603614 train.en.txt\n133317 train.vi.txt\n18074646 train.vi.txt\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"class SentencePieceTokenizer:\n    def __init__(self, model_path, add_special_tokens=True):\n        # self.sp = spm.SentencePieceProcessor()\n        # self.sp.load(model_path)\n\n        # self.pad_id = self.sp.pad_id()\n        # self.bos_id = self.sp.bos_id()\n        # self.eos_id = self.sp.eos_id()\n        # self.unk_id = self.sp.unk_id()\n\n        import pickle\n        with open(model_path, \"rb\") as f:\n            self.sp = pickle.load(f) # Load cái tokenizer tự viết lên\n            \n        self.bos_id = self.sp.bos_id\n        self.eos_id = self.sp.eos_id\n        self.pad_id = self.sp.pad_id\n        self.default_add_special_tokens = add_special_tokens\n\n    def encode(self, text, add_special_tokens=True):\n        # ids = self.sp.encode(text, out_type=int)\n        # if add_special_tokens:\n        #     ids = [self.bos_id] + ids + [self.eos_id]\n        # return ids\n        if add_special_tokens is None:\n            use_special = self.default_add_special_tokens\n        else:\n            use_special = add_special_tokens\n        ids = self.sp.encode(text) \n        \n        if use_special:\n            ids = [self.bos_id] + ids + [self.eos_id]\n        return ids\n        \n    def decode(self, ids):\n        ids = [i for i in ids if i not in\n               {self.pad_id, self.bos_id, self.eos_id}]\n        return self.sp.decode(ids)\n\n    def decode_until_eos(self, ids):\n        sent = []\n        for i in ids:\n            if i == self.eos_id:\n                break\n            if i in (self.pad_id, self.bos_id):\n                continue\n            sent.append(i)\n            \n        text = self.sp.decode(sent)\n        return self.detokenize(text)\n    \n    def detokenize(self,text):\n        # Ví dụ: \"Khi tôi còn nhỏ ,\" -> \"Khi tôi còn nhỏ,\"\n        text = text.replace(' ,', ',').replace(' .', '.')\n        text = text.replace(' !', '!').replace(' ?', '?')\n        text = text.replace(' :', ':').replace(' ;', ';')\n        return text\n\n    def vocab_size(self):\n        # return self.sp.get_piece_size()\n        return len(self.sp.vocab)","metadata":{"execution":{"iopub.status.busy":"2025-12-23T10:13:25.601639Z","iopub.execute_input":"2025-12-23T10:13:25.602445Z","iopub.status.idle":"2025-12-23T10:13:25.611582Z","shell.execute_reply.started":"2025-12-23T10:13:25.602411Z","shell.execute_reply":"2025-12-23T10:13:25.610871Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# s = \"công nghiệp hóa đất nước.\"a\n# ids = tgt_tokenizer.encode(s)\n# print(ids)\n# print(tgt_tokenizer.decode(ids))\n# print(tgt_tokenizer.decode_until_eos(ids))","metadata":{"execution":{"iopub.status.busy":"2025-12-23T10:13:28.633445Z","iopub.execute_input":"2025-12-23T10:13:28.634171Z","iopub.status.idle":"2025-12-23T10:13:28.637371Z","shell.execute_reply.started":"2025-12-23T10:13:28.634143Z","shell.execute_reply":"2025-12-23T10:13:28.636620Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"# Train sentencePiece","metadata":{}},{"cell_type":"code","source":"# %%time\n# import sentencepiece as spm\n\n# spm.SentencePieceTrainer.train(\n#     input='train.en.txt',\n#     model_prefix='spm_en',\n#     vocab_size=15000,\n#     model_type='unigram',\n#     character_coverage=1.0,\n#     hard_vocab_limit=False,\n#     bos_id=1,\n#     eos_id=2,\n#     pad_id=0,\n#     unk_id=3,\n# )\n\n# spm.SentencePieceTrainer.train(\n#     input='train.vi.txt',\n#     model_prefix='spm_vi',\n#     vocab_size=15000,\n#     model_type='unigram',\n#     character_coverage=0.9995,\n#     hard_vocab_limit=False,\n#     bos_id=1,\n#     eos_id=2,\n#     pad_id=0,\n#     unk_id=3,\n# )\n\n# Khởi tạo và train cho Tiếng Anh\nmy_spm_en = BPETokenizerFromScratch(vocab_size=15000) # Giảm vocab_size xuống chút cho nhanh nếu chạy python\nmy_spm_en.train('train.en.txt')\n\n# Khởi tạo và train cho Tiếng Việt\nmy_spm_vi = BPETokenizerFromScratch(vocab_size=15000)\nmy_spm_vi.train('train.vi.txt')\n\n# Lưu ý: dung pickle de luu .pkl cua ram\nimport pickle\nwith open(\"spm_en_custom.pkl\", \"wb\") as f:\n    pickle.dump(my_spm_en, f)\nwith open(\"spm_vi_custom.pkl\", \"wb\") as f:\n    pickle.dump(my_spm_vi, f)","metadata":{"execution":{"iopub.status.busy":"2025-12-23T10:13:31.152024Z","iopub.execute_input":"2025-12-23T10:13:31.152656Z","iopub.status.idle":"2025-12-23T10:52:57.174663Z","shell.execute_reply.started":"2025-12-23T10:13:31.152629Z","shell.execute_reply":"2025-12-23T10:52:57.173896Z"},"trusted":true},"outputs":[{"name":"stdout","text":"--- Đang training BPE trên file train.en.txt ---\nIter 100/14996: Merged ('u', 'r') -> ur\nIter 200/14996: Merged ('m', 'or') -> mor\nIter 300/14996: Merged ('in', 'e</w>') -> ine</w>\nIter 400/14996: Merged ('pro', 'b') -> prob\nIter 500/14996: Merged ('n', 'ing</w>') -> ning</w>\nIter 600/14996: Merged ('tion', 'al</w>') -> tional</w>\nIter 700/14996: Merged ('i', 'mag') -> imag\nIter 800/14996: Merged ('d', 'one</w>') -> done</w>\nIter 900/14996: Merged ('ma', 'y') -> may\nIter 1000/14996: Merged ('sm', 'all</w>') -> small</w>\nIter 1100/14996: Merged ('d', 'ra') -> dra\nIter 1200/14996: Merged ('grap', 'h') -> graph\nIter 1300/14996: Merged ('ic', 'k') -> ick\nIter 1400/14996: Merged ('mo', 'ther</w>') -> mother</w>\nIter 1500/14996: Merged ('st', 'ed</w>') -> sted</w>\nIter 1600/14996: Merged ('str', 'y</w>') -> stry</w>\nIter 1700/14996: Merged ('your', 'self</w>') -> yourself</w>\nIter 1800/14996: Merged ('su', 'g') -> sug\nIter 1900/14996: Merged ('N', 'A</w>') -> NA</w>\nIter 2000/14996: Merged ('I', 's') -> Is\nIter 2100/14996: Merged ('m', 'os') -> mos\nIter 2200/14996: Merged ('innov', 'ation</w>') -> innovation</w>\nIter 2300/14996: Merged ('g', 'ames</w>') -> games</w>\nIter 2400/14996: Merged ('tri', 'bu') -> tribu\nIter 2500/14996: Merged ('con', 'struc') -> construc\nIter 2600/14996: Merged ('an', 'e</w>') -> ane</w>\nIter 2700/14996: Merged ('bra', 'ins</w>') -> brains</w>\nIter 2800/14996: Merged ('z', 'er') -> zer\nIter 2900/14996: Merged ('well', '-') -> well-\nIter 3000/14996: Merged ('con', 'st') -> const\nIter 3100/14996: Merged ('g', 'ent') -> gent\nIter 3200/14996: Merged ('prac', 'tice</w>') -> practice</w>\nIter 3300/14996: Merged ('d', 'ad</w>') -> dad</w>\nIter 3400/14996: Merged ('Eur', 'op') -> Europ\nIter 3500/14996: Merged ('su', 'sp') -> susp\nIter 3600/14996: Merged ('anc', 'ed</w>') -> anced</w>\nIter 3700/14996: Merged ('Bra', 'z') -> Braz\nIter 3800/14996: Merged ('cy', 'cle</w>') -> cycle</w>\nIter 3900/14996: Merged ('deter', 'min') -> determin\nIter 4000/14996: Merged ('ta', 'il</w>') -> tail</w>\nIter 4100/14996: Merged ('la', 'un') -> laun\nIter 4200/14996: Merged ('industri', 'al</w>') -> industrial</w>\nIter 4300/14996: Merged ('coll', 'ect</w>') -> collect</w>\nIter 4400/14996: Merged ('opti', 'mis') -> optimis\nIter 4500/14996: Merged ('spec', 'tr') -> spectr\nIter 4600/14996: Merged ('on', 'ed</w>') -> oned</w>\nIter 4700/14996: Merged ('f', 'ell</w>') -> fell</w>\nIter 4800/14996: Merged ('disci', 'pl') -> discipl\nIter 4900/14996: Merged ('tu', 'm</w>') -> tum</w>\nIter 5000/14996: Merged ('ab', 'str') -> abstr\nIter 5100/14996: Merged ('spectr', 'um</w>') -> spectrum</w>\nIter 5200/14996: Merged ('person', 'ally</w>') -> personally</w>\nIter 5300/14996: Merged ('ann', 'oun') -> announ\nIter 5400/14996: Merged ('L', 'ast</w>') -> Last</w>\nIter 5500/14996: Merged ('gu', 'i') -> gui\nIter 5600/14996: Merged ('ma', 'gn') -> magn\nIter 5700/14996: Merged ('de', 'partment</w>') -> department</w>\nIter 5800/14996: Merged ('bor', 'ders</w>') -> borders</w>\nIter 5900/14996: Merged ('un', 'known</w>') -> unknown</w>\nIter 6000/14996: Merged ('perform', 'ing</w>') -> performing</w>\nIter 6100/14996: Merged ('hous', 'ing</w>') -> housing</w>\nIter 6200/14996: Merged ('pl', 'ain</w>') -> plain</w>\nIter 6300/14996: Merged ('occur', 'red</w>') -> occurred</w>\nIter 6400/14996: Merged ('dra', 'matically</w>') -> dramatically</w>\nIter 6500/14996: Merged ('embed', 'ded</w>') -> embedded</w>\nIter 6600/14996: Merged ('li', 's') -> lis\nIter 6700/14996: Merged ('o', 'id</w>') -> oid</w>\nIter 6800/14996: Merged ('imag', 'ery</w>') -> imagery</w>\nIter 6900/14996: Merged ('de', 'gra') -> degra\nIter 7000/14996: Merged ('d', 'ys') -> dys\nIter 7100/14996: Merged ('copy', 'right</w>') -> copyright</w>\nIter 7200/14996: Merged ('di', 've</w>') -> dive</w>\nIter 7300/14996: Merged ('stri', 'al</w>') -> strial</w>\nIter 7400/14996: Merged ('anc', 'ing</w>') -> ancing</w>\nIter 7500/14996: Merged ('photograph', 'ed</w>') -> photographed</w>\nIter 7600/14996: Merged ('arri', 've</w>') -> arrive</w>\nIter 7700/14996: Merged ('cu', 'te</w>') -> cute</w>\nIter 7800/14996: Merged ('fi', 'sh') -> fish\nIter 7900/14996: Merged ('head', 'lines</w>') -> headlines</w>\nIter 8000/14996: Merged ('magne', 'tic</w>') -> magnetic</w>\nIter 8100/14996: Merged ('wi', 'dely</w>') -> widely</w>\nIter 8200/14996: Merged ('ecosy', 'stems</w>') -> ecosystems</w>\nIter 8300/14996: Merged ('surr', 'ounding</w>') -> surrounding</w>\nIter 8400/14996: Merged ('my', 'ce') -> myce\nIter 8500/14996: Merged ('reve', 'als</w>') -> reveals</w>\nIter 8600/14996: Merged ('competi', 'tive</w>') -> competitive</w>\nIter 8700/14996: Merged ('pan', 'demic</w>') -> pandemic</w>\nIter 8800/14996: Merged ('im', 'men') -> immen\nIter 8900/14996: Merged ('shri', 'mp</w>') -> shrimp</w>\nIter 9000/14996: Merged ('psycho', 'logist</w>') -> psychologist</w>\nIter 9100/14996: Merged ('Tas', 'mani') -> Tasmani\nIter 9200/14996: Merged ('appro', 'ved</w>') -> approved</w>\nIter 9300/14996: Merged ('estab', 'lished</w>') -> established</w>\nIter 9400/14996: Merged ('be', 'e') -> bee\nIter 9500/14996: Merged ('pub', 'lish</w>') -> publish</w>\nIter 9600/14996: Merged ('st', 'es</w>') -> stes</w>\nIter 9700/14996: Merged ('A', 'pri') -> Apri\nIter 9800/14996: Merged ('impre', 'ssive</w>') -> impressive</w>\nIter 9900/14996: Merged ('pe', 'op') -> peop\nIter 10000/14996: Merged ('W', 'ar') -> War\nIter 10100/14996: Merged ('min', 'im') -> minim\nIter 10200/14996: Merged ('pen', 'is</w>') -> penis</w>\nIter 10300/14996: Merged ('de', 'pic') -> depic\nIter 10400/14996: Merged ('Cour', 't</w>') -> Court</w>\nIter 10500/14996: Merged ('dum', 'b</w>') -> dumb</w>\nIter 10600/14996: Merged ('terrori', 'sts</w>') -> terrorists</w>\nIter 10700/14996: Merged ('comm', 'and') -> command\nIter 10800/14996: Merged ('Jon', 'athan</w>') -> Jonathan</w>\nIter 10900/14996: Merged ('re', 'ef</w>') -> reef</w>\nIter 11000/14996: Merged ('d', 'ome</w>') -> dome</w>\nIter 11100/14996: Merged ('al', 'arm</w>') -> alarm</w>\nIter 11200/14996: Merged ('accep', 'tance</w>') -> acceptance</w>\nIter 11300/14996: Merged ('stic', 'ky</w>') -> sticky</w>\nIter 11400/14996: Merged ('John', 'son</w>') -> Johnson</w>\nIter 11500/14996: Merged ('hand', 'ful</w>') -> handful</w>\nIter 11600/14996: Merged ('labor', 'atories</w>') -> laboratories</w>\nIter 11700/14996: Merged ('L', 'oc') -> Loc\nIter 11800/14996: Merged ('sna', 'ke</w>') -> snake</w>\nIter 11900/14996: Merged ('k', 'er') -> ker\nIter 12000/14996: Merged ('Apoll', 'o</w>') -> Apollo</w>\nIter 12100/14996: Merged ('D', 'eath</w>') -> Death</w>\nIter 12200/14996: Merged ('it', 'ally</w>') -> itally</w>\nIter 12300/14996: Merged ('wa', 'shed</w>') -> washed</w>\nIter 12400/14996: Merged ('ur', 'us</w>') -> urus</w>\nIter 12500/14996: Merged ('facul', 'ty</w>') -> faculty</w>\nIter 12600/14996: Merged ('char', 'ts</w>') -> charts</w>\nIter 12700/14996: Merged ('Al', '-') -> Al-\nIter 12800/14996: Merged ('Ar', 'chit') -> Archit\nIter 12900/14996: Merged ('spher', 'es</w>') -> spheres</w>\nIter 13000/14996: Merged ('E', 'ss') -> Ess\nIter 13100/14996: Merged ('dis', 'appears</w>') -> disappears</w>\nIter 13200/14996: Merged ('Gar', 'den</w>') -> Garden</w>\nIter 13300/14996: Merged ('Con', 'nec') -> Connec\nIter 13400/14996: Merged ('plan', 'etary</w>') -> planetary</w>\nIter 13500/14996: Merged ('illi', 'ter') -> illiter\nIter 13600/14996: Merged ('pollut', 'ants</w>') -> pollutants</w>\nIter 13700/14996: Merged ('en', 'vy</w>') -> envy</w>\nIter 13800/14996: Merged ('contro', 'll') -> controll\nIter 13900/14996: Merged ('Cher', 'no') -> Cherno\nIter 14000/14996: Merged ('D', 'ata</w>') -> Data</w>\nIter 14100/14996: Merged ('I', 'l') -> Il\nIter 14200/14996: Merged ('re', 'create</w>') -> recreate</w>\nIter 14300/14996: Merged ('holi', 'day</w>') -> holiday</w>\nIter 14400/14996: Merged ('pra', 'yer</w>') -> prayer</w>\nIter 14500/14996: Merged ('A', 'la') -> Ala\nIter 14600/14996: Merged ('F', 'ul') -> Ful\nIter 14700/14996: Merged ('prop', 'h') -> proph\nIter 14800/14996: Merged ('gi', 'fted</w>') -> gifted</w>\nIter 14900/14996: Merged ('as', 'sau') -> assau\n--- Training hoàn tất! ---\n--- Đang training BPE trên file train.vi.txt ---\nIter 100/14996: Merged ('a', 'ng</w>') -> ang</w>\nIter 200/14996: Merged ('ế', 'u</w>') -> ếu</w>\nIter 300/14996: Merged ('c', 'uộc</w>') -> cuộc</w>\nIter 400/14996: Merged ('i', 'ễ') -> iễ\nIter 500/14996: Merged ('đ', 'o') -> đo\nIter 600/14996: Merged ('ph', 'ân</w>') -> phân</w>\nIter 700/14996: Merged ('lu', 'ôn</w>') -> luôn</w>\nIter 800/14996: Merged ('kh', 'iến</w>') -> khiến</w>\nIter 900/14996: Merged ('&', 'ap') -> &ap\nIter 1000/14996: Merged ('b', 'ố</w>') -> bố</w>\nIter 1100/14996: Merged ('i', 't') -> it\nIter 1200/14996: Merged ('ch', 'iều</w>') -> chiều</w>\nIter 1300/14996: Merged ('l', 'ái</w>') -> lái</w>\nIter 1400/14996: Merged ('C', 'o') -> Co\nIter 1500/14996: Merged ('l', 'inh</w>') -> linh</w>\nIter 1600/14996: Merged ('r', 'ơi</w>') -> rơi</w>\nIter 1700/14996: Merged ('V', 'ấn</w>') -> Vấn</w>\nIter 1800/14996: Merged ('l', 'ọc</w>') -> lọc</w>\nIter 1900/14996: Merged ('d', 'iệt</w>') -> diệt</w>\nIter 2000/14996: Merged ('c', 'ôn</w>') -> côn</w>\nIter 2100/14996: Merged ('Davi', 'd</w>') -> David</w>\nIter 2200/14996: Merged ('v', 'ườn</w>') -> vườn</w>\nIter 2300/14996: Merged ('tr', 'ắc</w>') -> trắc</w>\nIter 2400/14996: Merged ('ợ', 'n</w>') -> ợn</w>\nIter 2500/14996: Merged ('Tr', 'ường</w>') -> Trường</w>\nIter 2600/14996: Merged ('C', 'ộng</w>') -> Cộng</w>\nIter 2700/14996: Merged ('v', 'ắc</w>') -> vắc</w>\nIter 2800/14996: Merged ('T', 'in</w>') -> Tin</w>\nIter 2900/14996: Merged ('c', 'ừu</w>') -> cừu</w>\nIter 3000/14996: Merged ('m', 'òn</w>') -> mòn</w>\nIter 3100/14996: Merged ('T', 'ổ</w>') -> Tổ</w>\nIter 3200/14996: Merged ('gi', 'ặt</w>') -> giặt</w>\nIter 3300/14996: Merged ('ox', 'y') -> oxy\nIter 3400/14996: Merged ('ti', 'vi</w>') -> tivi</w>\nIter 3500/14996: Merged ('H', 'en') -> Hen\nIter 3600/14996: Merged ('T', 'es') -> Tes\nIter 3700/14996: Merged ('đ', 'ụng</w>') -> đụng</w>\nIter 3800/14996: Merged ('s', 'ửng</w>') -> sửng</w>\nIter 3900/14996: Merged ('đo', 'm</w>') -> đom</w>\nIter 4000/14996: Merged ('ch', 'iêm</w>') -> chiêm</w>\nIter 4100/14996: Merged ('T', 'im</w>') -> Tim</w>\nIter 4200/14996: Merged ('La', 'tin</w>') -> Latin</w>\nIter 4300/14996: Merged ('T', 'iền</w>') -> Tiền</w>\nIter 4400/14996: Merged ('kh', 'ốc</w>') -> khốc</w>\nIter 4500/14996: Merged ('chu', 'i</w>') -> chui</w>\nIter 4600/14996: Merged ('l', 'ót</w>') -> lót</w>\nIter 4700/14996: Merged ('T', 'anz') -> Tanz\nIter 4800/14996: Merged ('Mat', 't</w>') -> Matt</w>\nIter 4900/14996: Merged ('ẽ', 'o</w>') -> ẽo</w>\nIter 5000/14996: Merged ('T', 'ạp</w>') -> Tạp</w>\nIter 5100/14996: Merged ('w', 'ar') -> war\nIter 5200/14996: Merged ('tu', 're</w>') -> ture</w>\nIter 5300/14996: Merged ('qu', 'ấy</w>') -> quấy</w>\nIter 5400/14996: Merged ('oo', 'h</w>') -> ooh</w>\nIter 5500/14996: Merged ('v', 'ôi</w>') -> vôi</w>\nIter 5600/14996: Merged ('l', 's</w>') -> ls</w>\nIter 5700/14996: Merged ('Lu', 'ôn</w>') -> Luôn</w>\nIter 5800/14996: Merged ('đ', 'ấ') -> đấ\nIter 5900/14996: Merged ('ch', 'anh</w>') -> chanh</w>\nIter 6000/14996: Merged ('Phili', 'p</w>') -> Philip</w>\nIter 6100/14996: Merged ('Tr', 'ăng</w>') -> Trăng</w>\nIter 6200/14996: Merged ('Z', '</w>') -> Z</w>\nIter 6300/14996: Merged ('ph', 'ai</w>') -> phai</w>\nIter 6400/14996: Merged ('G', 'ay') -> Gay\nIter 6500/14996: Merged ('co', 'mp') -> comp\nIter 6600/14996: Merged ('An', 'thony</w>') -> Anthony</w>\nIter 6700/14996: Merged ('L', 'ựa</w>') -> Lựa</w>\nIter 6800/14996: Merged ('Mahmou', 'd</w>') -> Mahmoud</w>\nIter 6900/14996: Merged ('T', 'uyên</w>') -> Tuyên</w>\nIter 7000/14996: Merged ('M', 'ao</w>') -> Mao</w>\nIter 7100/14996: Merged ('as', 'e</w>') -> ase</w>\nIter 7200/14996: Merged ('Mah', 'atma</w>') -> Mahatma</w>\nIter 7300/14996: Merged ('Tricerato', 'ps</w>') -> Triceratops</w>\nIter 7400/14996: Merged ('D', 'ARPA</w>') -> DARPA</w>\nIter 7500/14996: Merged ('tion', 's</w>') -> tions</w>\nIter 7600/14996: Merged ('Na', 'om') -> Naom\nIter 7700/14996: Merged ('D', 'eb') -> Deb\nIter 7800/14996: Merged ('ch', 'ao</w>') -> chao</w>\nIter 7900/14996: Merged ('Ad', 'ri') -> Adri\nIter 8000/14996: Merged ('K', 'it') -> Kit\nIter 8100/14996: Merged ('oun', 'ds</w>') -> ounds</w>\nIter 8200/14996: Merged ('tr', 'ino</w>') -> trino</w>\nIter 8300/14996: Merged ('Re', 'ddit</w>') -> Reddit</w>\nIter 8400/14996: Merged ('Mc', 'Gon') -> McGon\nIter 8500/14996: Merged ('L', 'as</w>') -> Las</w>\nIter 8600/14996: Merged ('l', 'ẻo</w>') -> lẻo</w>\nIter 8700/14996: Merged ('z', 'y') -> zy\nIter 8800/14996: Merged ('Kh', 'ai</w>') -> Khai</w>\nIter 8900/14996: Merged ('awes', 'ome</w>') -> awesome</w>\nIter 9000/14996: Merged ('Zuri', 'ch</w>') -> Zurich</w>\nIter 9100/14996: Merged ('P', 'av') -> Pav\nIter 9200/14996: Merged ('Z', 'u') -> Zu\nIter 9300/14996: Merged ('Car', 'ib') -> Carib\nIter 9400/14996: Merged ('Elec', 'trolux</w>') -> Electrolux</w>\nIter 9500/14996: Merged ('yloid-', 'beta</w>') -> yloid-beta</w>\nIter 9600/14996: Merged ('Projec', 't</w>') -> Project</w>\nIter 9700/14996: Merged ('ng', 'le</w>') -> ngle</w>\nIter 9800/14996: Merged ('P', 'la') -> Pla\nIter 9900/14996: Merged ('Sim', 'pson</w>') -> Simpson</w>\nIter 10000/14996: Merged ('B', 'ran') -> Bran\nIter 10100/14996: Merged ('Di', 'ane</w>') -> Diane</w>\nIter 10200/14996: Merged ('Bei', 'j') -> Beij\nIter 10300/14996: Merged ('Inter', 'na') -> Interna\nIter 10400/14996: Merged ('Ar', 'n') -> Arn\nIter 10500/14996: Merged ('Me', 'h') -> Meh\nIter 10600/14996: Merged ('Kh', 'ảo</w>') -> Khảo</w>\nIter 10700/14996: Merged ('es', 'on</w>') -> eson</w>\nIter 10800/14996: Merged ('chú', 't.</w>') -> chút.</w>\nIter 10900/14996: Merged ('-', 'ni-') -> -ni-\nIter 11000/14996: Merged ('con', 'ta') -> conta\nIter 11100/14996: Merged ('Gri', 'er</w>') -> Grier</w>\nIter 11200/14996: Merged ('co', 'ban</w>') -> coban</w>\nIter 11300/14996: Merged ('xi-', 'lanh</w>') -> xi-lanh</w>\nIter 11400/14996: Merged ('V', 'ân</w>') -> Vân</w>\nIter 11500/14996: Merged ('Tr', 'ue</w>') -> True</w>\nIter 11600/14996: Merged ('th', 'ri') -> thri\nIter 11700/14996: Merged ('Y', 'e') -> Ye\nIter 11800/14996: Merged ('Cru', 'ise</w>') -> Cruise</w>\nIter 11900/14996: Merged ('D', 'ọc</w>') -> Dọc</w>\nIter 12000/14996: Merged ('Playst', 'ation</w>') -> Playstation</w>\nIter 12100/14996: Merged ('Bor', 'ges</w>') -> Borges</w>\nIter 12200/14996: Merged ('Blueto', 'oth</w>') -> Bluetooth</w>\nIter 12300/14996: Merged ('du', 'ệ</w>') -> duệ</w>\nIter 12400/14996: Merged ('DIY', 'bi') -> DIYbi\nIter 12500/14996: Merged ('kil', 'ô</w>') -> kilô</w>\nIter 12600/14996: Merged ('ph', 'ễu</w>') -> phễu</w>\nIter 12700/14996: Merged ('C', 'ặp</w>') -> Cặp</w>\nIter 12800/14996: Merged ('tro', 'l</w>') -> trol</w>\nIter 12900/14996: Merged ('cia-', 'Colombo</w>') -> cia-Colombo</w>\nIter 13000/14996: Merged ('Ri', 'ft</w>') -> Rift</w>\nIter 13100/14996: Merged ('Hem', 'ing') -> Heming\nIter 13200/14996: Merged ('ri', 'ch</w>') -> rich</w>\nIter 13300/14996: Merged ('Pier', 'pont</w>') -> Pierpont</w>\nIter 13400/14996: Merged ('Kry', 'p') -> Kryp\nIter 13500/14996: Merged ('nu', 't</w>') -> nut</w>\nIter 13600/14996: Merged ('ng', 'ỗ</w>') -> ngỗ</w>\nIter 13700/14996: Merged ('đú', 'ng.</w>') -> đúng.</w>\nIter 13800/14996: Merged ('Yen', 't') -> Yent\nIter 13900/14996: Merged ('An', 'ato') -> Anato\nIter 14000/14996: Merged ('pas', 'te') -> paste\nIter 14100/14996: Merged ('T', 'MS</w>') -> TMS</w>\nIter 14200/14996: Merged ('xk', 'c') -> xkc\nIter 14300/14996: Merged ('n', 'úp</w>') -> núp</w>\nIter 14400/14996: Merged ('T', 'C</w>') -> TC</w>\nIter 14500/14996: Merged ('Mugab', 'e</w>') -> Mugabe</w>\nIter 14600/14996: Merged ('Am', 'i</w>') -> Ami</w>\nIter 14700/14996: Merged ('S', 'BI') -> SBI\nIter 14800/14996: Merged ('cạc-', 'tông</w>') -> cạc-tông</w>\nIter 14900/14996: Merged ('Har', 't</w>') -> Hart</w>\n--- Training hoàn tất! ---\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"#!head spm_en.vocab\n\nsrc_tokenizer = SentencePieceTokenizer(\"spm_en_custom.pkl\")  # EN\ntgt_tokenizer = SentencePieceTokenizer(\"spm_vi_custom.pkl\")  # VI\n\nprint(\"SRC vocab size:\", src_tokenizer.vocab_size())\n\nprint(\"SRC example ids:\", src_tokenizer.encode(\"I love you\"))\nprint(\"SRC example txt:\", src_tokenizer.decode_until_eos(src_tokenizer.encode(\"I love you\")))\n\nprint('-'*80)\n# !head spm_vi.vocab\nprint(\"TGT vocab size:\", tgt_tokenizer.vocab_size())\nprint(\"TGT example ids:\", tgt_tokenizer.encode(\"Tôi yêu bạn\"))\nprint(\"TGT example txt:\", tgt_tokenizer.decode_until_eos(tgt_tokenizer.encode(\"Tôi yêu bạn\")))","metadata":{"execution":{"iopub.status.busy":"2025-12-23T10:55:33.208336Z","iopub.execute_input":"2025-12-23T10:55:33.208646Z","iopub.status.idle":"2025-12-23T10:55:33.256549Z","shell.execute_reply.started":"2025-12-23T10:55:33.208615Z","shell.execute_reply":"2025-12-23T10:55:33.255844Z"},"trusted":true},"outputs":[{"name":"stdout","text":"SRC vocab size: 14802\nSRC example ids: [1, 8411, 6178, 6255, 12901, 3822, 1618, 2]\nSRC example txt: I love you\n--------------------------------------------------------------------------------\nTGT vocab size: 14159\nTGT example ids: [1, 2969, 715, 7121, 1827, 2]\nTGT example txt: Tôi yêu bạn\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"# Pretrain embedding skip-gram\nTrain skip-gram -> save embedding -> load vào encoder -> freeze 2 epoch -> unfreeze -> train MT bình thường","metadata":{}},{"cell_type":"code","source":"# %%writefile pretrain_embedding.py\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import Counter\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n#from config import *\n\ndef build_unigram_table_sp(txt_file, sp_tokenizer, vocab_size, power=0.75):\n    counter = Counter()\n\n    with open(txt_file, encoding=\"utf-8\") as f:\n        for line in f:\n            ids = sp_tokenizer.encode(line, add_special_tokens=False)\n            counter.update(ids)\n\n    freqs = np.ones(vocab_size)\n    for idx, cnt in counter.items():\n        freqs[idx] = cnt\n\n    freqs[sp_tokenizer.pad_id] = 0\n    freqs[sp_tokenizer.bos_id] = 0\n    freqs[sp_tokenizer.eos_id] = 0\n    freqs[sp_tokenizer.unk_id] = 0\n\n    probs = freqs ** power\n    probs /= probs.sum()\n    return torch.tensor(probs, dtype=torch.float)\n\nclass SkipGramSPDataset(Dataset):\n    def __init__(self, txt_file, sp_tokenizer, window_size=2):\n        self.sentences = []\n        self.window = window_size\n        self.sp = sp_tokenizer\n\n        with open(txt_file, encoding=\"utf-8\") as f:\n            for line in f:\n                ids = self.sp.encode(line, add_special_tokens=False)\n                if len(ids) > 1:\n                    self.sentences.append(ids)\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        sent = self.sentences[idx]\n        i = torch.randint(0, len(sent), (1,)).item()\n\n        center = sent[i]\n        j = torch.randint(\n            max(0, i - self.window),\n            min(len(sent), i + self.window + 1),\n            (1,)\n        ).item()\n\n        if i == j:\n            if i + 1 < len(sent):\n                j = i + 1\n            else:\n                j = i - 1\n\n\n        return torch.tensor(center), torch.tensor(sent[j])\n\n\nclass SkipGramModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim):\n        super().__init__()\n        self.in_embed = nn.Embedding(vocab_size, embed_dim)\n        self.out_embed = nn.Embedding(vocab_size, embed_dim)\n\n        nn.init.xavier_uniform_(self.in_embed.weight)\n        nn.init.xavier_uniform_(self.out_embed.weight)\n\n    def forward(self, center, context, negatives):\n        \"\"\"\n        center:    [B]\n        context:   [B]\n        negatives: [B, K]\n        \"\"\"\n\n        v = self.in_embed(center)                  # [B, D]\n        u_pos = self.out_embed(context)            # [B, D]\n        u_neg = self.out_embed(negatives)          # [B, K, D]\n\n        # positive loss\n        pos_score = torch.sum(v * u_pos, dim=1)    # [B]\n        pos_loss = F.logsigmoid(pos_score)\n\n        # negative loss\n        neg_score = torch.bmm(u_neg, v.unsqueeze(2)).squeeze(2)  # [B, K]\n        neg_loss = F.logsigmoid(-neg_score).sum(dim=1)\n\n        return -(pos_loss + neg_loss).mean()\n\n\ndef train_skipgram_sp(\n    txt_file,\n    sp_tokenizer,\n    model_path,\n    embed_dim,\n    epochs=3,\n    batch_size=1024,\n    window=2,\n    neg_samples=5\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    dataset = SkipGramSPDataset(txt_file, sp_tokenizer, window)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    vocab_size = sp_tokenizer.vocab_size()\n    model = SkipGramModel(vocab_size, embed_dim).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n    unigram_probs = build_unigram_table_sp(\n        txt_file, sp_tokenizer, vocab_size\n    ).to(device)\n\n    for epoch in range(epochs):\n        total_loss = 0\n        for center, context in dataloader:\n            center = center.to(device)\n            context = context.to(device)\n\n            negatives = torch.multinomial(\n                unigram_probs,\n                center.size(0) * neg_samples,\n                replacement=True\n            ).view(center.size(0), neg_samples)\n\n            negatives[negatives == center.unsqueeze(1)] = sp_tokenizer.unk_id\n\n\n            optimizer.zero_grad()\n            loss = model(center, context, negatives)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f\"[SkipGram-SP] Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n\n    torch.save(\n        {\n            \"weight\": model.in_embed.weight.data.cpu(),\n            \"vocab_size\": vocab_size,\n            \"embed_dim\": embed_dim\n        },\n        model_path\n    )\n\n    print(f\"Saved SP embeddings to {model_path}\")\n\nif __name__ == \"__main__\":\n    train_skipgram_sp(\n        train_data_path + \"train.en.txt\",\n        src_tokenizer,\n        \"spm_en_skipgram.pt\",\n        SKIPGRAM_DIM,\n        epochs=SKIPGRAM_EPOCHS\n    )\n    \n    train_skipgram_sp(\n        train_data_path + \"train.vi.txt\",\n        tgt_tokenizer,\n        \"spm_vi_skipgram.pt\",\n        SKIPGRAM_DIM,\n        epochs=SKIPGRAM_EPOCHS\n    )","metadata":{"execution":{"iopub.status.busy":"2025-12-23T10:56:42.471879Z","iopub.execute_input":"2025-12-23T10:56:42.472221Z","iopub.status.idle":"2025-12-23T10:56:42.492284Z","shell.execute_reply.started":"2025-12-23T10:56:42.472194Z","shell.execute_reply":"2025-12-23T10:56:42.491403Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2919914513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_unigram_table_sp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'"],"ename":"ModuleNotFoundError","evalue":"No module named 'config'","output_type":"error"}],"execution_count":40},{"cell_type":"code","source":"def freeze_embeddings(model):\n    # Token embeddings\n    model.encoder.emb.tok_emb.weight.requires_grad = False\n    model.decoder.embedding.tok_emb.weight.requires_grad = False\n\ndef unfreeze_embeddings(model):\n    model.encoder.emb.tok_emb.weight.requires_grad = True\n    model.decoder.embedding.tok_emb.weight.requires_grad = True\n\ndef load_pretrained_embedding(embedding_layer, path):\n    state = torch.load(path, map_location=\"cpu\")\n    assert embedding_layer.weight.shape == state[\"weight\"].shape\n    embedding_layer.weight.data.copy_(state[\"weight\"])\n\n# load_pretrained_embedding(model.encoder.emb.tok_emb, \"spm_en_skipgram.pt\")\n# load_pretrained_embedding(model.decoder.embedding.tok_emb, \"spm_vi_skipgram.pt\")","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.397410Z","iopub.status.idle":"2025-12-23T08:55:52.397721Z","shell.execute_reply.started":"2025-12-23T08:55:52.397573Z","shell.execute_reply":"2025-12-23T08:55:52.397590Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch\n\nclass TranslationDataset(Dataset):\n    def __init__(self, src_file, tgt_file, src_tokenizer, tgt_tokenizer, max_len):\n        self.src_lines = open(src_file, encoding='utf-8').read().splitlines()\n        self.tgt_lines = open(tgt_file, encoding='utf-8').read().splitlines()\n        # self.tokenizer = tokenizer\n        self.src_tokenizer = src_tokenizer\n        self.tgt_tokenizer = tgt_tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.src_lines)\n\n    def __getitem__(self, idx):\n        src = self.src_tokenizer.encode(self.src_lines[idx])[:self.max_len]\n        tgt = self.tgt_tokenizer.encode(self.tgt_lines[idx])[:self.max_len]\n        return torch.tensor(src), torch.tensor(tgt)\n        \ndef collate_fn(batch, src_pad_id, tgt_pad_id):\n    src_batch, tgt_batch = zip(*batch)\n\n    src_batch = torch.nn.utils.rnn.pad_sequence(\n        src_batch, padding_value=src_pad_id, batch_first=True\n    )\n    tgt_batch = torch.nn.utils.rnn.pad_sequence(\n        tgt_batch, padding_value=tgt_pad_id, batch_first=True\n    )\n    return src_batch, tgt_batch\n\n","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.398731Z","iopub.status.idle":"2025-12-23T08:55:52.399041Z","shell.execute_reply.started":"2025-12-23T08:55:52.398869Z","shell.execute_reply":"2025-12-23T08:55:52.398884Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\n\nclass NoamScheduler:\n    def __init__(self, optimizer, d_model, warmup_steps):\n        self.optimizer = optimizer\n        self.d_model = d_model\n        self.warmup_steps = warmup_steps\n        self.step_num = 0\n\n    def step(self):\n        self.step_num += 1\n        lr = self.get_lr()\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n        self.optimizer.step()\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n    def get_lr(self):\n        return (self.d_model ** -0.5) * min(\n            self.step_num ** -0.5,\n            self.step_num * (self.warmup_steps ** -1.5)\n        )\n","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.400326Z","iopub.status.idle":"2025-12-23T08:55:52.400611Z","shell.execute_reply.started":"2025-12-23T08:55:52.400491Z","shell.execute_reply":"2025-12-23T08:55:52.400506Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # Assumptions\n# d_model = 512\n# warmup_steps = 4000\n# steps_per_epoch = 1000\n# epochs = 30\n# total_steps = steps_per_epoch * epochs\n\n# # Noam learning rate function\n# def noam_lr(step, d_model, warmup_steps):\n#     return (d_model ** -0.5) * min(\n#         step ** -0.5,\n#         step * (warmup_steps ** -1.5)\n#     )\n\n# # Compute learning rates\n# steps = list(range(1, total_steps + 1))\n# lrs = [noam_lr(step, d_model, warmup_steps) for step in steps]\n\n# # High-quality plot\n# plt.figure(figsize=(8, 5), dpi=400)     # tăng kích thước & độ phân giải\n# plt.plot(steps, lrs, linewidth=2)       # đường vẽ dày hơn\n# plt.yscale(\"log\")                       # log-scale để thấy rõ warmup + decay\n# plt.xlabel(\"Training Step\", fontsize=12)\n# plt.ylabel(\"Learning Rate\", fontsize=12)\n# plt.title(\"Noam Learning Rate Scheduler\", fontsize=13)\n# # plt.grid(True, linestyle=\"--\", alpha=0.4)\n\n# plt.savefig('lr_scheduler.png', dpi=400)  # 300 dpi\n\n# plt.tight_layout()\n# plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.402593Z","iopub.status.idle":"2025-12-23T08:55:52.402977Z","shell.execute_reply.started":"2025-12-23T08:55:52.402786Z","shell.execute_reply":"2025-12-23T08:55:52.402809Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from pyvi.ViTokenizer import ViTokenizer\n# from keras.src.legacy.preprocessing.text import Tokenizer\n# from keras.src.utils import pad_sequences\n\n# Đọc dữ liệu từ tệp\ndef load_data(en_file, vi_file):\n    with open(en_file, 'r', encoding='utf-8') as f:\n        en_data = f.read().strip().split(\"\\n\")\n    with open(vi_file, 'r', encoding='utf-8') as f:\n        vi_data = f.read().strip().split(\"\\n\")\n    return en_data, vi_data\n\ndef get_tokenize(data, add_start_end=False):\n    # Khởi tạo Tokenizer\n    tokenizer = Tokenizer(filters='', oov_token=UNKNOWN_TOKEN)\n    if (add_start_end):\n        tokenizer.fit_on_texts([START_TOKEN, END_TOKEN] + data)\n    else:\n        tokenizer.fit_on_texts(data)\n    return data, tokenizer\n\ndef get_tokenize_seq(en_data, vi_data, en_tokenizer, vi_tokenizer, max_sequence_length):\n    en_data = [f\"{START_TOKEN} {sentence} {END_TOKEN}\" for sentence in en_data]\n    en_sequences = en_tokenizer.texts_to_sequences(en_data)\n\n    vi_data = [ViTokenizer.tokenize(sentence) for sentence in vi_data]\n    vi_sequences = vi_tokenizer.texts_to_sequences(vi_data)\n\n    filtered_en = []\n    filtered_vi = []\n    # Giữ lại những câu có số từ <= max_sequence_length\n    for i in range(len(en_sequences)):\n        if (len(en_sequences[i]) <= max_sequence_length) and (len(vi_sequences[i]) <= max_sequence_length):\n            filtered_en.append(en_sequences[i])\n            filtered_vi.append(vi_sequences[i])\n\n    filtered_en = torch.tensor(pad_sequences(filtered_en, maxlen=max_sequence_length, padding='post'), dtype=torch.long)\n    filtered_vi = torch.tensor(pad_sequences(filtered_vi, maxlen=max_sequence_length, padding='post'), dtype=torch.long)\n\n    return filtered_en, filtered_vi\n\n# Tiền xử lý dữ liệu\ndef preprocess_tokenizer(en_data, vi_data):\n    en_data, en_tokenizer = get_tokenize(en_data, add_start_end=True)\n\n    vi_data = [ViTokenizer.tokenize(sentence) for sentence in vi_data]\n    vi_data, vi_tokenizer = get_tokenize(vi_data)\n\n    return en_tokenizer, vi_tokenizer\n\ndef preprocess_data(train_src_path, train_trg_path, val_src_path, val_trg_path):\n    # Load dữ liệu\n    en_data, vi_data = load_data(train_src_path, train_trg_path)\n    en_data_val, vi_data_val = load_data(val_src_path, val_trg_path)\n\n    en_tokenizer, vi_tokenizer = preprocess_tokenizer(en_data, vi_data)\n\n    en_sequences, vi_sequences = get_tokenize_seq(en_data, vi_data, en_tokenizer, vi_tokenizer,\n                                                  max_sequence_length=MAX_SEQ_LEN)\n    en_val_sequences, vi_val_sequences = get_tokenize_seq(en_data_val, vi_data_val, en_tokenizer, vi_tokenizer,\n                                                          max_sequence_length=MAX_SEQ_LEN)\n\n    all_train_sequences = list(zip(vi_sequences, en_sequences))\n    all_val_sequences = list(zip(vi_val_sequences, en_val_sequences))\n\n    return en_tokenizer, vi_tokenizer, all_train_sequences, all_val_sequences\n\ndef merge_sentences(text, max_seq_length):\n    sentences = [s.strip() for s in text.split(\",\")]  # Tách câu và xóa khoảng trắng dư thừa\n\n    merged = []\n    temp = \"\"\n    word_count = 0\n\n    for sentence in sentences:\n        words = sentence.split()  # Đếm số từ trong câu hiện tại\n        if word_count + len(words) <= max_seq_length:\n            temp = temp + \", \" + sentence if temp else sentence  # Nối câu\n            word_count += len(words)  # Cập nhật số từ\n        else:\n            merged.append(temp)  # Lưu câu hiện tại vào danh sách\n            temp = sentence  # Bắt đầu câu mới\n            word_count = len(words)  # Reset số từ\n\n    if temp:  # Đừng quên thêm câu cuối cùng\n        merged.append(temp)\n\n    return merged\n","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.405036Z","iopub.status.idle":"2025-12-23T08:55:52.405660Z","shell.execute_reply.started":"2025-12-23T08:55:52.405438Z","shell.execute_reply":"2025-12-23T08:55:52.405464Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import nn\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.attention = ScaleDotProductAttention()\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_concat = nn.Linear(d_model, d_model)\n\n    def forward(self, query, key, value, mask=None):\n        # 1. dot product with weight matrices\n        query, key, value = self.w_q(query), self.w_k(key), self.w_v(value)\n\n        # 2. split tensor by number of heads\n        query, key, value = self.split(query), self.split(key), self.split(value)\n\n        # 3. do scale dot product to compute similarity\n        out, attention = self.attention(query, key, value, mask=mask)\n\n        # 4. concat and pass to linear layer\n        out = self.concat(out)\n        out = self.w_concat(out)\n\n        # 5. visualize attention map\n        # TODO : we should implement visualization\n        return out\n\n    def split(self, tensor):\n        batch_size, length, d_model = tensor.size()\n\n        d_tensor = d_model // self.num_heads\n        tensor = tensor.view(batch_size, length, self.num_heads, d_tensor).transpose(1, 2)\n        # it is similar with group convolution (split by number of heads)\n\n        return tensor\n\n    def concat(self, tensor):\n        batch_size, num_heads, length, d_tensor = tensor.size()\n        d_model = d_tensor * self.num_heads\n\n        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n        return tensor","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.406914Z","iopub.status.idle":"2025-12-23T08:55:52.407353Z","shell.execute_reply.started":"2025-12-23T08:55:52.407194Z","shell.execute_reply":"2025-12-23T08:55:52.407220Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import _LRScheduler\n\nclass CustomLearningRateSchedule(_LRScheduler):\n    def __init__(self, optimizer, initial_lr, decay_rates, decay_steps, lr_decay_interval, last_epoch=-1):\n        \"\"\"\n        initial_lr: Learning rate ban đầu\n        decay_rates: Danh sách hệ số decay (n phần tử)\n        decay_steps: Danh sách step ứng với decay (n-1 phần tử)\n        lr_decay_interval: Khoảng cách giữa các lần decay\n        \"\"\"\n        assert len(decay_rates) - 1 == len(decay_steps), \"Số lượng decay_steps phải ít hơn decay_rates một phần tử\"\n\n        self.initial_lr = initial_lr\n        self.decay_rates = decay_rates\n        self.decay_steps = decay_steps\n        self.lr_decay_interval = lr_decay_interval\n        self.prev_decay_step = 0\n\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        step = self.last_epoch\n        lr = self.initial_lr\n        prev_decay_step = 0\n\n        # Áp dụng các decay ban đầu\n        for i in range(len(self.decay_steps)):\n            decay_factor = self.decay_rates[i]\n            num_intervals = max((min(step, self.decay_steps[i]) - prev_decay_step) // self.lr_decay_interval, 0)\n            lr *= decay_factor ** num_intervals\n            prev_decay_step = self.decay_steps[i]\n\n        # Áp dụng decay cuối cùng mãi mãi\n        decay_factor = self.decay_rates[-1]\n        num_intervals = max((step - prev_decay_step) // self.lr_decay_interval, 0)\n        lr *= decay_factor ** num_intervals\n\n        return [lr for _ in self.base_lrs]  # Trả về danh sách cho từng group của optimizer\n\n    def state_dict(self):\n        return {\n            \"initial_lr\": self.initial_lr,\n            \"decay_rates\": self.decay_rates,\n            \"decay_steps\": self.decay_steps,\n            \"lr_decay_interval\": self.lr_decay_interval,\n            \"prev_decay_step\": self.prev_decay_step\n        }\n\n    def load_state_dict(self, state_dict):\n        self.initial_lr = state_dict[\"initial_lr\"]\n        self.decay_rates = state_dict[\"decay_rates\"]\n        self.decay_steps = state_dict[\"decay_steps\"]\n        self.lr_decay_interval = state_dict[\"lr_decay_interval\"]\n        self.prev_decay_step = state_dict[\"prev_decay_step\"]","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.408882Z","iopub.status.idle":"2025-12-23T08:55:52.409291Z","shell.execute_reply.started":"2025-12-23T08:55:52.409071Z","shell.execute_reply":"2025-12-23T08:55:52.409096Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport math\n\nclass ScaleDotProductAttention(nn.Module):\n    def __init__(self):\n        super(ScaleDotProductAttention, self).__init__()\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, query, key, value, mask=None):\n        # input is 4 dimension tensor\n        # [batch_size, num_heads, length, d_tensor]\n        batch_size, num_heads, length, d_tensor = key.size()\n\n        # 1. dot product Query with Key^T to compute similarity\n        key_t = key.transpose(2, 3)\n        score = (query @ key_t) / math.sqrt(d_tensor)\n\n        # 2. apply masking (opt)\n        if mask is not None:\n            score = score.masked_fill(mask == 0, -100000000)\n\n        # 3. pass them softmax to make [0, 1] range\n        score = self.softmax(score)\n\n        # 4. multiply with Value\n        value = score @ value\n\n        return value, score\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len, device):\n        \"\"\"\n           constructor of sinusoid encoding class\n\n           :param d_model: dimension of model\n           :param max_len: max sequence length\n           :param device: hardware device setting\n        \"\"\"\n        super(PositionalEncoding, self).__init__()\n\n        # same size with input matrix (for adding with input matrix)\n        self.encoding = torch.zeros(max_len, d_model, device=device)\n        self.encoding.requires_grad = False # we don't need to compute gradient\n\n        pos = torch.arange(0, max_len, device=device)\n        pos = pos.float().unsqueeze(dim=1)\n\n        _2i = torch.arange(0, d_model, 2, device=device).float()\n\n        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n        # compute positional encoding to consider positional information of words\n\n    def forward(self, x):\n        batch_size, seq_len = x.size()\n        return self.encoding[:seq_len, :]\n\n# class PositionwiseFeedForward(nn.Module):\n#     def __init__(self, d_model, d_ff, dropout):\n#         super(PositionwiseFeedForward, self).__init__()\n#         self.linear1 = nn.Linear(d_model, d_ff)\n#         self.linear2 = nn.Linear(d_ff, d_model)\n#         self.relu = nn.ReLU()\n#         self.dropout = nn.Dropout(dropout)\n\n#     def forward(self, x):\n#         x = self.linear1(x)\n#         x = self.relu(x)\n#         x = self.dropout(x)\n#         x = self.linear2(x)\n#         return x\n\nimport torch.nn.functional as F\nclass SwiGLUFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout):\n        super().__init__()\n        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n        self.w2 = nn.Linear(d_model, d_ff, bias=False)\n        self.w3 = nn.Linear(d_ff, d_model, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w3(\n            self.dropout(\n                F.silu(self.w2(x)) * self.w1(x)\n            )\n        )\n        \n\nclass TransformerEmbedding(nn.Module):\n    def __init__(self, vocab_size, d_model, max_len, dropout, device, pad_idx, skipgram_dim):\n        super(TransformerEmbedding, self).__init__()\n        self.tok_emb = nn.Embedding(vocab_size, skipgram_dim, padding_idx=pad_idx)\n        self.proj = nn.Linear(skipgram_dim, d_model)\n        self.pos_emb = PositionalEncoding(d_model, max_len, device)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        tok = self.proj(self.tok_emb(x))\n        pos = self.pos_emb(x)\n        return self.dropout(tok + pos)\n        \n        # tok_emb = self.tok_emb(x)\n        # pos_emb = self.pos_emb(x)\n        # zreturn self.dropout(tok_emb + pos_emb)\n","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.410436Z","iopub.status.idle":"2025-12-23T08:55:52.411483Z","shell.execute_reply.started":"2025-12-23T08:55:52.411268Z","shell.execute_reply":"2025-12-23T08:55:52.411287Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import nn\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, d_ff, num_heads, dropout):\n        super(EncoderLayer, self).__init__()\n        self.attention = MultiHeadAttention(d_model, num_heads)\n        self.norm1 = nn.LayerNorm(d_model, eps=EPS)\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.ffn = SwiGLUFeedForward(d_model, d_ff, dropout)\n        self.norm2 = nn.LayerNorm(d_model, eps=EPS)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x, src_mask):\n        # 1. compute self attention\n        _x = x\n        x = self.attention(x, x, x, src_mask)\n\n        # 2. add and norm\n        x = self.dropout1(x)\n        x = self.norm1(_x + x)\n\n        # 3. positionwise feed forward network\n        _x = x\n        x = self.ffn(x)\n\n        # 4. add and norm\n        x = self.dropout2(x)\n        x = self.norm2(_x + x)\n\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self, inp_vocab_size, max_len, d_model, d_ff, num_heads, num_layers, dropout, device, pad_idx, skipgram_dim):\n        super(Encoder, self).__init__()\n        self.emb = TransformerEmbedding(inp_vocab_size, d_model, max_len, dropout, device, pad_idx, skipgram_dim)\n        self.layers = nn.ModuleList([EncoderLayer(d_model, d_ff, num_heads, dropout) for _ in range(num_layers)])\n\n    def forward(self, src, src_mask):\n        x = self.emb(src)\n        for layer in self.layers:\n            x = layer(x, src_mask)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.412411Z","iopub.status.idle":"2025-12-23T08:55:52.412945Z","shell.execute_reply.started":"2025-12-23T08:55:52.412781Z","shell.execute_reply":"2025-12-23T08:55:52.412809Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import nn\n\nclass Decoder_Layer(nn.Module):\n    def __init__(self, d_model, d_ff, num_heads, dropout):\n        super(Decoder_Layer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.norm1 = nn.LayerNorm(d_model, eps=EPS)\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n        self.norm2 = nn.LayerNorm(d_model, eps=EPS)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.ffn = SwiGLUFeedForward(d_model, d_ff, DROPOUT)\n        self.norm3 = nn.LayerNorm(d_model, eps=EPS)\n        self.dropout3 = nn.Dropout(dropout)\n\n    def forward(self, x, enc_out, trg_mask, src_mask):\n        # 1. compute self attention\n        _x = x\n        x = self.self_attn(x, x, x, mask=trg_mask)\n\n        # 2. add and norm\n        x = self.dropout1(x)\n        x = self.norm1(_x + x)\n\n        if enc_out is not None:\n            # 3. compute encoder - decoder attention\n            _x = x\n            x = self.enc_dec_attn(x, enc_out, enc_out, mask=src_mask)\n\n            # 4. add and norm\n            x = self.dropout2(x)\n            x = self.norm2(_x + x)\n\n        # 5. positionwise feed forward network\n        _x = x\n        x = self.ffn(x)\n\n        # 6. add and norm\n        x = self.dropout3(x)\n        x = self.norm3(_x + x)\n\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, trg_vocab_size, max_len, d_model, d_ff, num_heads, num_layers, dropout, device, pad_idx, skipgram_dim):\n        super(Decoder, self).__init__()\n        self.embedding = TransformerEmbedding(trg_vocab_size, d_model, max_len, dropout, device, pad_idx, skipgram_dim)\n        self.layers = nn.ModuleList([Decoder_Layer(d_model, d_ff, num_heads, dropout) for i in range(num_layers)])\n        self.linear = nn.Linear(d_model, trg_vocab_size)\n\n    def forward(self, trg, enc_src, trg_mask, src_mask):\n        trg = self.embedding(trg)\n\n        for layer in self.layers:\n            trg = layer(trg, enc_src, trg_mask, src_mask)\n\n        # pass to LM head\n        output = self.linear(trg)\n\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.414264Z","iopub.status.idle":"2025-12-23T08:55:52.414587Z","shell.execute_reply.started":"2025-12-23T08:55:52.414451Z","shell.execute_reply":"2025-12-23T08:55:52.414472Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass Transformer(nn.Module):\n    def __init__(self, src_pad_idx, trg_pad_idx, inp_vocab_size, trg_vocab_size, d_model, num_heads, max_len, d_ff, num_layers, dropout, device, skipgram_dim):\n        super(Transformer, self).__init__()\n        self.src_pad_idx = src_pad_idx\n        self.trg_pad_idx = trg_pad_idx\n        self.device = device\n\n        self.encoder = Encoder(inp_vocab_size, max_len, d_model, d_ff, num_heads, num_layers, dropout, device, src_pad_idx, skipgram_dim)\n        self.decoder = Decoder(trg_vocab_size, max_len, d_model, d_ff, num_heads, num_layers, dropout, device, trg_pad_idx, skipgram_dim)\n\n    def forward(self, src, trg):\n        src_mask = self.make_src_mask(src)\n        trg_mask = self.make_trg_mask(trg)\n        enc_out = self.encoder(src, src_mask)\n        output = self.decoder(trg, enc_out, trg_mask, src_mask)\n        return output\n\n    def make_src_mask(self, src):\n        src_mask = (src != self.src_pad_idx).unsqueeze(dim=1).unsqueeze(dim=2)\n        return src_mask\n\n    def make_trg_mask(self, trg):\n        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(dim=1).unsqueeze(dim=3)\n        trg_len = trg.shape[1]\n        trg_look_ahead_mask = torch.tril(torch.ones(trg_len, trg_len)).bool().to(self.device)\n        trg_mask = trg_pad_mask & trg_look_ahead_mask\n\n        return trg_mask","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.415724Z","iopub.status.idle":"2025-12-23T08:55:52.415980Z","shell.execute_reply.started":"2025-12-23T08:55:52.415851Z","shell.execute_reply":"2025-12-23T08:55:52.415865Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nimport time\n\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef initialize_weights(m):\n    if hasattr(m, 'weight') and m.weight.dim() > 1:\n        nn.init.kaiming_uniform(m.weight.data)\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\nsrc_tokenizer = SentencePieceTokenizer(\"spm_en.model\")  # EN\ntgt_tokenizer = SentencePieceTokenizer(\"spm_vi.model\")  # VI\n\nSRC_PAD_ID = src_tokenizer.pad_id\nTGT_PAD_ID = tgt_tokenizer.pad_id\nSRC_VOCAB_SIZE = src_tokenizer.vocab_size()\nTGT_VOCAB_SIZE = tgt_tokenizer.vocab_size()\n\nprint(\"SRC_PAD_ID\",SRC_PAD_ID)\nprint(\"TGT_PAD_ID\",TGT_PAD_ID)\nprint(\"SRC_PAD token:\", src_tokenizer.sp.id_to_piece(SRC_PAD_ID))\nprint(\"TGT_PAD token:\", tgt_tokenizer.sp.id_to_piece(TGT_PAD_ID))\nprint(\"SRC_VOCAB_SIZE\",SRC_VOCAB_SIZE)\nprint(\"TGT_VOCAB_SIZE\",TGT_VOCAB_SIZE)\n\ntrain_dataset = TranslationDataset(\n    src_file=train_data_path + \"train.en.txt\",\n    tgt_file=train_data_path + \"train.vi.txt\",\n    src_tokenizer=src_tokenizer,\n    tgt_tokenizer=tgt_tokenizer,\n    max_len=MAX_SEQ_LEN\n)\n\nval_dataset = TranslationDataset(\n    src_file=data_path + \"tst2013.en.txt\",\n    tgt_file=data_path + \"tst2013.vi.txt\",\n    src_tokenizer=src_tokenizer,\n    tgt_tokenizer=tgt_tokenizer,\n    max_len=MAX_SEQ_LEN\n)\n\nif DEBUG:\n    train_dataset.src_lines = train_dataset.src_lines[:2000]\n    train_dataset.tgt_lines = train_dataset.tgt_lines[:2000]\n    val_dataset.src_lines = val_dataset.src_lines[:500]\n    val_dataset.tgt_lines = val_dataset.tgt_lines[:500]\n    EPOCHS = 5\n\ntrain_batches = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    collate_fn=lambda b: collate_fn(b, SRC_PAD_ID, TGT_PAD_ID)\n)\n\nval_batches = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    collate_fn=lambda b: collate_fn(b, SRC_PAD_ID, TGT_PAD_ID)\n)\n\n# Initializing model\nmodel = Transformer(\n    src_pad_idx=SRC_PAD_ID,\n    trg_pad_idx=TGT_PAD_ID,\n    inp_vocab_size=SRC_VOCAB_SIZE,\n    trg_vocab_size=TGT_VOCAB_SIZE,\n    d_model=D_MODEL,\n    num_heads=NUM_HEADS,\n    max_len=MAX_SEQ_LEN,\n    d_ff=D_FF,\n    num_layers=NUM_LAYERS,\n    dropout=DROPOUT,\n    device=DEVICE,\n    skipgram_dim = SKIPGRAM_DIM\n).to(DEVICE)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')\n\nprint(f'\\nBefore load_pretrained_embedding:')\nprint(model.encoder.emb.tok_emb.weight[:2, :5])\nprint(model.decoder.embedding.tok_emb.weight[:2, :5])\nprint(\"Projection:\", model.encoder.emb.proj)\n\nload_pretrained_embedding(model.encoder.emb.tok_emb, \"spm_en_skipgram.pt\")\nload_pretrained_embedding(model.decoder.embedding.tok_emb, \"spm_vi_skipgram.pt\")\n\nprint(f'\\nAfter load_pretrained_embedding:')\nprint(model.encoder.emb.tok_emb.weight[:2, :5])\nprint(model.decoder.embedding.tok_emb.weight[:2, :5])\nprint(\"Projection:\", model.encoder.emb.proj)\n\noptimizer = torch.optim.Adam(\n    # filter(lambda p: p.requires_grad, model.parameters()),\n    model.parameters(),\n    betas=BETAS,\n    eps=EPSILON\n)\n\nsteps_per_epoch = len(train_batches) # dataset 133k / bs 164 ~ 810\ntotal_steps = steps_per_epoch * EPOCHS\nwarmup_steps = int(total_steps*WARMUP_RATIO)\n\nprint(\"Total steps:\", total_steps)\nprint(\"Warmup steps:\", warmup_steps)\n\nscheduler = NoamScheduler(\n    optimizer,\n    d_model=D_MODEL,\n    warmup_steps= warmup_steps \n)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=TGT_PAD_ID,label_smoothing=0.1)","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.418080Z","iopub.status.idle":"2025-12-23T08:55:52.418780Z","shell.execute_reply.started":"2025-12-23T08:55:52.418564Z","shell.execute_reply":"2025-12-23T08:55:52.418590Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def greedy_decode(model, src, tgt_tokenizer, max_len):\n    \"\"\"\n    src: [B, S]\n    return: [B, T]\n    \"\"\"\n    model.eval()\n    batch_size = src.size(0)\n    device = src.device\n\n    bos_id = tgt_tokenizer.bos_id\n    eos_id = tgt_tokenizer.eos_id\n    pad_id = tgt_tokenizer.pad_id\n\n    # decoder input bắt đầu bằng <s>\n    ys = torch.full(\n        (batch_size, 1),\n        bos_id,\n        dtype=torch.long,\n        device=device\n    )\n\n    with torch.no_grad():\n        for _ in range(max_len - 1):\n            # logits: [B, T, vocab]\n            out = model(src, ys)\n\n            # lấy token cuối\n            next_token = out[:, -1, :].argmax(dim=-1)\n\n            ys = torch.cat(\n                [ys, next_token.unsqueeze(1)],\n                dim=1\n            )\n\n            # nếu tất cả đều EOS thì dừng\n            if (next_token == eos_id).all():\n                break\n\n    # pad cho đủ chiều (nếu cần)\n    if ys.size(1) < max_len:\n        pad = torch.full(\n            (batch_size, max_len - ys.size(1)),\n            pad_id,\n            dtype=torch.long,\n            device=device\n        )\n        ys = torch.cat([ys, pad], dim=1)\n\n    return ys\n","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.419761Z","iopub.status.idle":"2025-12-23T08:55:52.420431Z","shell.execute_reply.started":"2025-12-23T08:55:52.420285Z","shell.execute_reply":"2025-12-23T08:55:52.420304Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef beam_search_decode(\n    model,\n    src,\n    tgt_tokenizer,\n    max_len,\n    beam_size=BEAM_SIZE,\n    length_penalty=LENGTH_PENALTY\n):\n    \"\"\"\n    src: [B, S]\n    return: [B, T]\n    \"\"\"\n    model.eval()\n    device = src.device\n    batch_size = src.size(0)\n\n    bos_id = tgt_tokenizer.bos_id\n    eos_id = tgt_tokenizer.eos_id\n    pad_id = tgt_tokenizer.pad_id\n\n    # Encode source ONCE\n    with torch.no_grad():\n        src_mask = model.make_src_mask(src)\n        enc_out = model.encoder(src, src_mask)\n\n    outputs = []\n\n    for b in range(batch_size):\n        beams = [{\n            \"seq\": torch.tensor([[bos_id]], device=device),\n            \"score\": 0.0,\n            \"finished\": False\n        }]\n\n        for _ in range(max_len - 1):\n            candidates = []\n\n            for beam in beams:\n                if beam[\"finished\"]:\n                    candidates.append(beam)\n                    continue\n\n                ys = beam[\"seq\"]\n                trg_mask = model.make_trg_mask(ys)\n\n                with torch.no_grad():\n                    out = model.decoder(\n                        ys,\n                        enc_out[b:b+1],\n                        trg_mask,\n                        src_mask[b:b+1]\n                    )\n\n                log_probs = F.log_softmax(out[:, -1, :], dim=-1)\n                topk_log_probs, topk_ids = log_probs.topk(beam_size, dim=-1)\n\n                for k in range(beam_size):\n                    token = topk_ids[0, k].item()\n                    score = beam[\"score\"] + topk_log_probs[0, k].item()\n\n                    new_seq = torch.cat(\n                        [ys, torch.tensor([[token]], device=device)],\n                        dim=1\n                    )\n\n                    candidates.append({\n                        \"seq\": new_seq,\n                        \"score\": score,\n                        \"finished\": token == eos_id\n                    })\n\n            # length penalty + chọn top beam\n            for c in candidates:\n                lp = ((5 + c[\"seq\"].size(1)) / 6) ** length_penalty\n                c[\"norm_score\"] = c[\"score\"] / lp\n\n            beams = sorted(\n                candidates,\n                key=lambda x: x[\"norm_score\"],\n                reverse=True\n            )[:beam_size]\n\n            if all(bm[\"finished\"] for bm in beams):\n                break\n\n        best = beams[0][\"seq\"].squeeze(0)\n\n        if best.size(0) < max_len:\n            best = torch.cat([\n                best,\n                torch.full(\n                    (max_len - best.size(0),),\n                    pad_id,\n                    device=device\n                )\n            ])\n\n        outputs.append(best)\n\n    return torch.stack(outputs, dim=0)\n","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.421288Z","iopub.status.idle":"2025-12-23T08:55:52.421563Z","shell.execute_reply.started":"2025-12-23T08:55:52.421431Z","shell.execute_reply":"2025-12-23T08:55:52.421448Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sacrebleu\n\ndef evaluate_bleu(model, iterator):\n    model.eval()\n    hypotheses, references = [], []\n\n    with torch.no_grad():\n        for src, trg in iterator:\n            src = src.to(model.device)\n            trg = trg.to(model.device)\n\n            pred_sent = beam_search_decode(model, src, tgt_tokenizer, MAX_SEQ_LEN)\n            # pred_sent = greedy_decode(model, src, tgt_tokenizer, MAX_SEQ_LEN)\n\n            for b in range(pred_sent.size(0)):\n                hyp = tgt_tokenizer.decode_until_eos(pred_sent[b].tolist())\n                ref = tgt_tokenizer.decode_until_eos(trg[b].tolist())\n\n                if hyp.strip() and ref.strip():\n                    hypotheses.append(hyp)\n                    references.append(ref)  \n                    \n    if len(hypotheses) == 0:\n        bleu = 0.0\n        print(f'\\nHypotheses Invalid \\n')\n    else:\n        bleu = sacrebleu.corpus_bleu(hypotheses,[references],tokenize='13a').score                    \n        \n    return bleu\n\n#         # BLEU\n    #         pred_sent = greedy_decode(\n    #             model,\n    #             src,\n    #             tgt_tokenizer,\n    #             MAX_SEQ_LEN\n    #         )\n\n    #         for b in range(pred_sent.size(0)):\n    #             hyp = tgt_tokenizer.decode_until_eos(pred_sent[b].tolist())\n    #             ref = tgt_tokenizer.decode_until_eos(trg[b].tolist())\n    #             # print(\"REF:\", ref)\n    #             # print(\"HYP:\", hyp)       \n    #             # print(\"=\"*100)                \n    #             # print(len(hyp.split()), len(ref.split()))\n\n    #             if hyp.strip() and ref.strip():\n    #                 hypotheses.append(hyp)\n    #                 references.append(ref)\n\n    # if len(hypotheses) == 0:\n    #     bleu = 0.0\n    # else:\n    #     bleu = sacrebleu.corpus_bleu(hypotheses,[references],tokenize='13a').score\n\n    # return epoch_loss / len(iterator), total_correct / total_tokens, bleu\n    \n# final_bleu = evaluate_bleu(model, val_batches)","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.422816Z","iopub.status.idle":"2025-12-23T08:55:52.423077Z","shell.execute_reply.started":"2025-12-23T08:55:52.422953Z","shell.execute_reply":"2025-12-23T08:55:52.422968Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_metric(bleu, record):\n    print(\"=\" * 60)\n    print(\"FINAL EVALUATION RESULT (BEST MODEL)\")\n    print(\"=\" * 60)\n    print(f\"Epoch        : {record['epoch']}\")\n    print(f\"Train Loss     : {record['train_loss']:.4f}\")\n    print(f\"Val Loss     : {record['val_loss']:.4f}\")\n    print(f\"Train Accuracy : {record['train_accuracy']:.4f}\")\n    print(f\"Val Accuracy : {record['val_accuracy']:.4f}\")\n    print(f\"BLEU score   : {bleu:.2f}\")\n    print(f\"Train PPL      : {record['train_ppl']:.4f}\")\n    print(f\"Val PPL      : {record['val_ppl']:.4f}\")\n    \n","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.424149Z","iopub.status.idle":"2025-12-23T08:55:52.424419Z","shell.execute_reply.started":"2025-12-23T08:55:52.424293Z","shell.execute_reply":"2025-12-23T08:55:52.424308Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport csv\nimport os\n\ndef train(model, iterator, scheduler, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    for (i, (src, trg)) in enumerate(iterator):\n        src = src.to(model.device)  # Đưa src về cùng thiết bị với model\n        trg = trg.to(model.device)  # Đưa trg về cùng thiết bị với model\n        # optimizer.zero_grad()\n        scheduler.zero_grad()\n        \n        output = model(src, trg[:, :-1])\n        output_reshape = output.contiguous().view(-1, output.shape[-1])\n        trg_gold = trg[:, 1:].contiguous().view(-1)\n\n        loss = criterion(output_reshape, trg_gold)\n        loss.backward()\n        \n        # Tính norm của gradient trước khi clip\n        grad_norm_before = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n        # Clip gradient để tránh exploding gradient\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        # Tính norm của gradient sau khi clip\n        grad_norm_after = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n        # optimizer.step()\n        scheduler.step()\n\n        # Tính số lượng token đúng => Accuracy\n        pred = output.argmax(dim=-1).view(-1)  # Lấy token có xác suất cao nhất\n        # pred = output.argmax(dim=-1).contiguous().view(-1)\n        mask = (trg_gold != TGT_PAD_ID)  # Bỏ qua token padding\n        \n        correct = (pred == trg_gold ) & mask  # Đúng và không phải padding\n        total_correct += correct.sum().item()\n        total_tokens += mask.sum().item()\n        \n        epoch_loss += loss.item()\n        if (i + 1) % BATCH_PRINT == 0:\n            lr = optimizer.param_groups[0]['lr']\n            print(f'Batch: {i+1}/{len(iterator)}, Loss: {loss.item():.4f}, Accuracy: {total_correct / total_tokens:.4f}, LR: {lr:.6f}, '\n                  f'Grad Norm Before Clip: {grad_norm_before:.6f}, Grad Norm After Clip: {grad_norm_after:.6f}')\n            \n    return epoch_loss / len(iterator), total_correct / total_tokens\n\ndef evaluate(model, iterator, criterion):\n    model.eval()\n    epoch_loss = 0\n    total_correct = 0\n    total_tokens = 0\n\n    hypotheses,references = [], []\n    \n    with torch.no_grad():\n        for (i, (src, trg)) in enumerate(iterator):\n            \n            # print(\"src\", src)\n            # print(\"trg\", trg)\n            # print(\"TRG IDS RAW:\", trg[i][:20].tolist())\n\n            src = src.to(model.device)  # Đưa src về cùng thiết bị với model\n            trg = trg.to(model.device)  # Đưa trg về cùng thiết bị với model\n            #Forward\n            output = model(src, trg[:, :-1])\n            #Loss\n            output_reshape = output.contiguous().view(-1, output.shape[-1])\n            trg_gold  = trg[:, 1:].contiguous().view(-1)\n            loss = criterion(output_reshape, trg_gold.view(-1))\n            epoch_loss += loss.item()\n\n            # Accuracy\n            # pred = output.argmax(dim=-1).view(-1)  # Lấy token có xác suất cao nhất\n            pred = output.argmax(dim=-1).contiguous().view(-1)  # Lấy token có xác suất cao nhất\n            mask = (trg_gold != TGT_PAD_ID)  # Bỏ qua token padding\n            # print(\"PRED IDS 1:\", pred[i][:10].tolist())\n            \n            correct = (pred == trg_gold) & mask  # Đúng và không phải padding\n            total_correct += correct.sum().item()\n            total_tokens += mask.sum().item()\n     \n    return epoch_loss / len(iterator), total_correct / total_tokens\n    #         # BLEU\n    #         pred_sent = greedy_decode(\n    #             model,\n    #             src,\n    #             tgt_tokenizer,\n    #             MAX_SEQ_LEN\n    #         )\n\n    #         for b in range(pred_sent.size(0)):\n    #             hyp = tgt_tokenizer.decode_until_eos(pred_sent[b].tolist())\n    #             ref = tgt_tokenizer.decode_until_eos(trg[b].tolist())\n    #             # print(\"REF:\", ref)\n    #             # print(\"HYP:\", hyp)       \n    #             # print(\"=\"*100)                \n    #             # print(len(hyp.split()), len(ref.split()))\n\n    #             if hyp.strip() and ref.strip():\n    #                 hypotheses.append(hyp)\n    #                 references.append(ref)\n\n    # if len(hypotheses) == 0:\n    #     bleu = 0.0\n    # else:\n    #     bleu = sacrebleu.corpus_bleu(hypotheses,[references],tokenize='13a').score\n\n    # return epoch_loss / len(iterator), total_correct / total_tokens, bleu\n\nimport sacrebleu\n    \ndef run(total_epoch, best_loss):\n    train_losses, test_losses = [], []\n    objs = []\n    \n    best_model_path = None\n    best_record = None\n    \n    is_frozen = False\n    is_unfrozen = False\n    \n    for step in range(total_epoch):\n        # ===== Freeze / Unfreeze =====\n        if step < FREEZE_EPOCHS and not is_frozen:\n            freeze_embeddings(model)\n            is_frozen = True\n            print(f\"[Epoch {step+1}] 🔒 Freeze token embeddings\")\n        elif step >= FREEZE_EPOCHS and not is_unfrozen:\n            unfreeze_embeddings(model)\n            is_unfrozen = True\n            print(f\"[Epoch {step+1}] 🔓 Unfreeze token embeddings\")\n\n            \n        print(f'Epoch: {step + 1}')\n        start_time = time.time()\n        # ===== Train / Eval =====\n        train_loss, train_accuracy = train(model, train_batches, scheduler, criterion, CLIP)\n        val_loss, val_accuracy = evaluate(model, val_batches, criterion)\n        end_time = time.time()\n\n        train_losses.append(train_loss)\n        test_losses.append(val_loss)\n\n        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n        \n        # ===== Log record =====\n        log_record = {\n            \"epoch\": step + 1,\n            \"train_loss\": round(train_loss, 6),\n            \"train_accuracy\": round(train_accuracy, 6),\n            \"train_ppl\": round(math.exp(train_loss), 6),\n            \"val_loss\": round(val_loss, 6),\n            \"val_accuracy\": round(val_accuracy, 6),\n            # \"val_bleu\": round(val_bleu, 6),\n            \"val_ppl\": round(math.exp(val_loss), 6),\n            \"epoch_time_sec\": round(end_time - start_time, 2)\n        }\n\n        objs.append(log_record)\n        \n        # ===== Save BEST model =====\n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_record = log_record\n            best_model_path = f'{saved_model_path}/model-{val_loss:.3f}-{val_accuracy:.3f}.pt'\n            torch.save(model.state_dict(), best_model_path)\n            \n        # ===== Write logs =====\n        with open(JSON_LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n            json.dump(objs, f, indent=2, ensure_ascii=False)\n\n        with open(CSV_LOG_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n            writer = csv.DictWriter(f, fieldnames=CSV_FIELDS)\n            writer.writerow(log_record)\n    \n        #Console\n        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train Accuracy: {train_accuracy:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n        print(f'\\tVal Loss: {val_loss:.3f} | Val Accuracy: {val_accuracy:.3f} | Val PPL: {math.exp(val_loss):7.3f}')\n\n    bleu = evaluate_bleu(model,val_batches)\n\n    display_metric(bleu, best_record)\n\n    return best_model_path, best_record","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.425995Z","iopub.status.idle":"2025-12-23T08:55:52.426351Z","shell.execute_reply.started":"2025-12-23T08:55:52.426187Z","shell.execute_reply":"2025-12-23T08:55:52.426214Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, gc\ngc.collect(), torch.cuda.empty_cache()\nbest_model_path, best_record = run(total_epoch=EPOCHS, best_loss=float('inf'))\n\n# print(\"=\"*60)\n# print(f\"Loading: {best_model_path}\")\n\n# model.load_state_dict(torch.load(best_model_path, map_location=model.device))\n\n# final_bleu = evaluate_bleu(model, val_batches)\n\n# display_metric(final_bleu, best_record)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:55:52.427767Z","iopub.status.idle":"2025-12-23T08:55:52.428093Z","shell.execute_reply.started":"2025-12-23T08:55:52.427962Z","shell.execute_reply":"2025-12-23T08:55:52.427980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nEpoch: 1\nBatch: 100/813, Loss: 4.1780, Accuracy: 0.4097, LR: 0.000511, Grad Norm Before Clip: 0.856700, Grad Norm After Clip: 0.856700\nBatch: 200/813, Loss: 4.0969, Accuracy: 0.4142, LR: 0.000547, Grad Norm Before Clip: 0.857101, Grad Norm After Clip: 0.857101\nBatch: 300/813, Loss: 4.1269, Accuracy: 0.4181, LR: 0.000584, Grad Norm Before Clip: 1.244583, Grad Norm After Clip: 0.999999\nBatch: 400/813, Loss: 4.0409, Accuracy: 0.4227, LR: 0.000621, Grad Norm Before Clip: 0.926793, Grad Norm After Clip: 0.926793\nBatch: 500/813, Loss: 4.0945, Accuracy: 0.4269, LR: 0.000657, Grad Norm Before Clip: 0.891969, Grad Norm After Clip: 0.891969\nBatch: 600/813, Loss: 3.9800, Accuracy: 0.4310, LR: 0.000694, Grad Norm Before Clip: 0.831140, Grad Norm After Clip: 0.831140\nBatch: 700/813, Loss: 3.9321, Accuracy: 0.4349, LR: 0.000731, Grad Norm Before Clip: 0.894853, Grad Norm After Clip: 0.894853\nBatch: 800/813, Loss: 3.9520, Accuracy: 0.4383, LR: 0.000768, Grad Norm Before Clip: 0.932369, Grad Norm After Clip: 0.932369\nREF: Đây là vẻ ngoài của tôi , chụp cạnh bà của mình trước đó vài tháng .\nHYP: Đây là những gì tôi nhìn thấy với phòng thí nghiệm của mình chỉ là một vài năm trước .\n====================================================================================================\nREF: Đây là tôi trong cùng một ngày khi chụp bức ảnh trên .\nHYP: Đây là tôi đang ở một ngày như thế này .\n====================================================================================================\nREF: Cô bạn của tôi đã đi cùng tôi .\nHYP: Mẹ tôi phải trả lời cho tôi .\n====================================================================================================\nREF: Đây là tôi ở tiệc ngủ vài ngày trước khi chụp ảnh cho Vogue Pháp .\nHYP: Đây là tôi ở một vài năm trước khi tôi ngồi trước trước khi tôi tham gia .\n====================================================================================================\nREF: Đây là tôi với đội bóng đá trong tạp chí V.\nHYP: Đây là tôi đang ở trên sân khấu và ở New York .\n====================================================================================================\n\nEpoch: 1 | Time: 8m 54s\n\tTrain Loss: 4.031 | Train Accuracy: 0.439 | Train PPL:  56.303\n\tVal Loss: 3.786 | Val Accuracy: 0.493 | Val BLEU: 17.490 | Val PPL:  44.079\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2025-12-23T08:55:52.430013Z","iopub.status.idle":"2025-12-23T08:55:52.430354Z","shell.execute_reply.started":"2025-12-23T08:55:52.430172Z","shell.execute_reply":"2025-12-23T08:55:52.430188Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null}]}